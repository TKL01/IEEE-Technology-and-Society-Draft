\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{varioref}
\usepackage{cleveref}
%added xcolor for box color
\usepackage{xcolor}
\usepackage{enumitem}


\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}
	
	\title{Combining Multimodal Scene Graph and  Large Language Model  for Inclusive Human-Robot  Teanming in the Society of the Future}
	
	\author{Trung Kien La and Eric Guiffo Kaigom
		% <-this % stops a space
		\thanks{Trung Kien La was with the Frankfurt Industrial Robotics and Digital Twin Laboratory (FriiDA) at the Frankfurt University of Applied Sciences, Hungener Str. 6 Building C, 60389 Frankfurt am Main, Germany (Orcid: 0009-0007-9239-3268).}% <-this % stops a space
		\thanks{Eric Guiffo Kaigom is with the Frankfurt Industrial Robotics and Digital Twin Laboratory (FriiDA) at the Frankfurt University of Applied Sciences, Hungener Str. 6 Building C, 60389 Frankfurt am Main, Germany (e-mail: kaigom@fra-uas.de).}}
	
	% The paper headers
	\markboth{Replace this line with your manuscript id number}%
	{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}
	
	%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
	% Remember, if you use this you must call \IEEEpubidadjcol in the second
	% column for its text to clear the IEEEpubid mark.
	
	\maketitle
	
	\begin{abstract}
		People regularly need to reach and move loads in daily life. Many essential activities in the society build on it.  
		However, even a short distance and an increased physical exertion make especially people with limited motor and low visual abilities refrain from  these activities. Robots can help sustain self-determination and preserve health in this context. However,  robots are mostly developed for scripted motions and subject to a programming complexity, which is a barrier for their adoption. It remains hard to \textit{verbally instruct} a robot to reach a previously unseen target object in its workspace or to follow a human operator who situationally decides on the desired destination of the payload carried by the robot.  
		Conversely, robots seldom share their current perception and reasoning about the sensed environment with humans, failing to engage humans in understanding their mental models and adapting their   expectations from a robotic assistance or guidance. In this work, we describe the development of an infrastructure that combines a semantic object detection and segmentation based on YOLO to construct a scene graph   with a subsequent interpretation of the structured scene driven by Multimodal Large Language Models  for inclusive human-robot teaming. The meaningful representation of the operational space enables  a mobile payloaded robot to reach or track a designated target entity, such as a  chair or a human in motion. On the other hand, the robot communicates its perception of the environment along with feedback on task execution and its internals to the human operator using a natural language in real-time. Experiment results show that the extended robot intelligence simplifies the penetration of robots, facilitates  inclusion,  and fosters the self-determination of citizens in societal applications, paving the ground for an improved perceived usability and usefulness of robots in the daily life in the society of the future. 
	\end{abstract}
	
	\begin{IEEEkeywords}
		 Intelligent Robot, Multi-modal Large Language Models, YOLO, Scene Graph,  Extended Robot Intelligence, Society of the Future
	\end{IEEEkeywords}
	
	\section{Introduction}\label{introi}
	\IEEEPARstart{T}{he} society of the future will be enriched with emerging digital technologies that support the well-being of humans by enhancing    their inclusion and self-determination \cite{obrenovic2025generative,kaigom2023metarobotics}. 
	Learned collective intelligence driven by large language models (LLMs), such as GPT, LLaMA, Gemini and Claude, together with generative AI tools like ChatGPT  are increasingly fostering inclusion. For instance, access to globally shared information in a desired  natural language is being more and more possible. On the other side, mechanically assisted motor skills provided by e.g. wearable robots  for physically demanding tasks are witnessing notable advances. However, there are    limitations when it comes to carry heavy payloads over a long distance without muscular or ergonomic risks for humans \cite{singh2024pushing}. Furthermore, the integration of  robotic devices in   the collective intelligence loop  to support  self-determination  through natural  interactions between humans and robots (see \cref{intro})  has received little attention. Reasons span from  fundamental automation principles for robots that still require a high programming effort to the lack of intuitive interfaces that bring robots close to citizens in terms of pervasiveness, usability, and augmentation of capacities in societal tasks \cite{kaigom2023metarobotics}.
	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth]{pic/movingbox.png}
		\caption{Healthy payload handling without stress over a long distance using a  mobile robot with extended intelligence. The robot carries the payload, builds and shares a mental scene graph-based model to reason about its environment, autonomously avoids collisions, and  follows the human operator to the intended location while giving contextualized feedback in real-time.}
		\label{intro}
	\end{figure}
	
	While mobile robots are useful to carry a wide range of payloads without physical stress \cite{gona2024intelligent}, citizens struggle to ask them to move to a target location. Doing so for everyone requires a paradigm shift from programming-based to communication-oriented task achievement that hides the technical complexity and emphasizes  societal opportunities and usefulness. An  application example is to  move a heavy box from a place to another without workspace calibration, in which case the robot is steered to a desired position by following a human operator as shown in \cref{intro}. The operator can freely and comfortably move around and specify the  final position of the payload in motion anywhere. The robot simply tracks the uncertain human path while avoiding  obstacles to reach that position. Nevertheless, most current mobile robots fail to communicate their mental model, i.e., understanding of the operational environment, difficulty to achieve a goal, and internal state, to humans in real-time. This bidirectional gap unfortunately prevents citizens to  engage in a service-oriented dialog with the robot to anticipate events, strategically refine  their expectations in an iterative way,  or adapt  their individual \textit{requests} (instead of hard-coded \textit{commands}) to the robot on the fly without writing any line of code.
	
	The immediate advantages of doing so are manifold for a human teammate interaction with the robot. The communication with the robot can occur in  a desired language supported by an LLM. Individual needs can be conveyed more readily.  A technical  background is no longer necessary to prompt the robot to complete a task. People can translate complex tasks into  sequences of single spoken request primitives to achieve specific objectives. For example, following a complex path can be broken down into  intermediate motions of the robot that aim to attain the vicinity of previously unseen objects that suddenly appear in the workspace of the moving robot  (see \cref{mental}). A  human teammate in motion  can act as a guide followed upon invitation by the robot that serves as a assistive device  carrying the payload to a desired location. Meanwhile, the robot can exploit its reasoning about its dynamic and experienced mental model of the environment to amend human guidance (e.g., in the case of events grounded in historical contexts or long-term decisions) and give directive feedback about the environment to human. This shared autonomy democratizes  access to robots and facilitates the  inclusion as well as the self-determination of human operators. 
	
	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth]{pic/mental.png}
		\caption{Perception of an \textit{indoor} environment by a mobile robot. Semantic detection and segmentation of objects to construct a scene graph as a mental model. The robot reasons on this graph using an MLLM to track a moving chair described by the human-teammate using natural language.}
		\label{mental}
	\end{figure}
	
	
	We describe in this work the development of an infrastructure for code-free teaming between human and mobile robot that helps complete various tasks in societal applications.  Included are indoor assistance, in which case the robot acts as a commissioned  agent and follower for synergistic payload transport. Everyone can  benefit from this functionality and shape it to own specific motor, vision, and communication needs regardless of the technical background. Furthermore, the robot  pro-actively offers outdoor guidance that can be helpful for individuals with e.g. motor or visual impairments. Our main contributions are as follows.% 
	\begin{itemize}
		\item We integrate a YOLO-driven object inference  from 2D images  together with spatial information from a depth camera sensor and measured data on the robot internals to construct a dynamic multi-modal scene graph. It provides the  mobile robot with a hierarchical,   semantic, and \textit{self-aware} abstract representation of its current workspace and inner processes that it harness as a mental model. 
		\item The robot then leverages  Multimodal Large Language Models (MLLM) to understand and reason about the scene graph to answer human requests and own thoughts. Any citizen can \textit{tell} the payloaded robot which physical entity  it should follow or which location it should reach. The environment- and self-aware robot makes assistive and directive decisions which are  communicated to the human-teammate using a natural language.
		\item We demonstrate the usefulness and performance of our extended robot intelligence for cooperative task execution based on a communication-based shared autonomy between human and robot in three societal applications. 
	\end{itemize}
The remainder of this work is organized as follows.

	\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth]{pic/mental2.png}
	\caption{Semantic understanding of the \textit{outdoor} operational environment by the robot. In this application, the robot not only follows the human teammate, but also give hints about surrounding objects using a natural language. Objects are e.g. obstacles or  useful items like a bicycle. The robot is  aware of an estimated relative pose  (i.e., position and orientation) and velocity along with the affordance of objects even unseen by humans. It uses this information for human guidance and reachability assessment upon request. Hence, the robot employs AI to perceive the scene (we coined it \textit{pAIrSEEption}) for the humans.}
	\label{mental}
\end{figure}





\section{Related Work}
Robotized societal applications are typically characterized by unstructured environments, such as home settings, where scenes exhibit diverse spatial, temporal, and contextual dynamics. These dynamics are shaped by routine human activities as well as irregularities and uncertainties, for example, arising from human intentions \cite{yalcinkaya2024towards}. Constructing a scene graph for such environments requires the identification, localization, and interpretation of entities—termed nodes—along with their attributes and the relationships between them, which are represented as edges \cite{rana2023sayplan}. The ability of statistical methods based on neural networks to handle uncertainty and generalize across scenarios has promoted their use in object detection and scene graph generation for mobile robotics. In the next sections, we provide an overview of related architectures and methods developed to this end.
 


 
\subsection{Layered architecture for scene graph}
Mobile robots need to reason about contextualized and structured information to plan and achieve goals. Scene graphs have recently been adapted for mobile robots by introducing different layers of abstraction of the real scene. These include the spatial (locations within the scene), semantic (meaning of entities, including their state, affordances, predicates, and further attributes), topological (connectivity between locations), and task-related (robot goals) layers \cite{rana2023sayplan,im2024egtr}. Those layers are populated with nodes and differ in various ways. The update rates of node attributes are one example of such differentiation \cite{rana2023sayplan,im2024egtr}. For instance, while the initial and final positions of the robot may be known in advance and thus fixed, task achievement may be expected within a short time window, which requires fast motions. By contrast, the desired position of the robot varies with time in object-following applications.
  \begin{table}[!t]
	\centering
	\caption{Techniques for scene graph generation and  characteristics.}
	\label{tab:features}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Contribution} & \textbf{Feature representation} & \textbf{Relation inference method} \\
		\hline \hline
		(\cite{kim2025semantic,su2024adopting}) & Box-based &  MPNN    \\
		\hline
		(\cite{im2024egtr,hao2025bctr}) & Query-based &  Attention  \\
		\hline
		(\cite{liu2021fully,newell2017pixels}) & Point-based &  CNN \\
		\hline
	\end{tabular}
\end{table}

\begin{table*}[!hb]
	\centering
	\caption{Technical Overview of Frameworks for 3D Scene Graph Generation, Perception, Navigation and Interfacing in Robotics}
	\label{tab:scenegraph_technologies}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{System/Framework} & \textbf{\begin{tabular}[c]{@{}l@{}}Detection \& Segmentation \\ (Nodes)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Semantic Features \& \\ Embeddings\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Relation Inference \& \\ Planning (Edges)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Alternatives \& \\ Additional Models \& \\ Misc. \end{tabular}}  \\ \hline
			\textbf{ConceptGraphs}\cite{conceptgraphs} & SAM & CLIP, Grounding DINO & GPT-4, LLaVA & RAM \\ \hline
			\textbf{3DGraphLLM}\cite{3dgraphllm} & Mask3D, OneFormer3D & DINOv2 (2D), Uni3D (3D) & VL-SAT, CLIP & LLaMA3-8B, Vicuna-1.5 \\ \hline
			\textbf{Graph2Nav}\cite{graph2nav} & PSGFormer, Mask2Former & Panoptic Labels & 2D-PSG Inference, GPT-3.5 & LIO-SAM, Whisper \\ \hline
			\textbf{HOV-SG}\cite{hovsg} & SAM & CLIP & Hierarchical Heuristics & GPT-3.5, GPT-4 \\ \hline
			\textbf{OpenIN}\cite{openin} & CropFormer & SBERT, CLIP & GPT-4o & Tokenize Anything \\ \hline
			\textbf{SG-Nav}\cite{sgnav} & SAM & VLMs, LLMs & LLaMA-7B, GPT-4, LLaVA & Fast Marching Method \\ \hline
			\textbf{Interaction Updates}\cite{interaction} & Existing 3D SG (e.g., Hydra) & Visual Transformer (ViT) & GPT-4, BLIP-2 & DBSCAN (Denoising) \\ \hline
			\textbf{VisionGPT}\cite{visiongpt} & YOLO-World & Proportional 2D Coordinates & GPT-3.5 Turbo, GPT-4 & H-Pattern Splitter \\ \hline
			\textbf{LGX}\cite{lgx} & GLIP, YOLO & BLIP (Captions) & GPT-3 & OWL-ViT, Detectron2 \\ \hline
			\textbf{VLMaps}\cite{vlmaps} & LSeg & CLIP & Codex (Code-writing LLM) & RTAB-Map \\ \hline
		\end{tabular}%
	}
\end{table*}

\subsection{Algorithms for the generation of scene graphs}
Three classes of algorithms emerge for the generation of scene graphs.
Edge dual scene graph generation has been recently proposed   to capture multi-object relationships for scene graph generation \cite{kim2025semantic}. The approach integrates object detection based on a fast Region-based Convolutional Neural Networks (R-CNN), an edge dual scene graph,  and an object-relation-centric Message Passing Neural Network (MPNN). Rich contextual interactions  between unconstrained objects are retrieved by the object-relation MPNN that learns both object- and relation-centric features to enhance the prediction accuracy and enable  fine-grained and iterative updates between objects. In essence, MPNN aggregates information from neighborhood nodes. This enables the update of the description of the considered node. MPNN finally outputs a feature vector for the entire graph \cite{kim2025semantic,su2024adopting}. Another technique for graph generation leverages the attention mechanism. The  approach for instance connects objects with significant attention weights to edges from the self-attention layers of a pre-trained DEtection TRansformer  \cite{im2024egtr}. In the third Convolutional Scene Graph Generation (CSGG) strategy, objects are encoded as bounding box center points. Relationships are represented e.g. as 2D vector fields, i.e., an integral on a sub-region that points from subject to object \cite{liu2021fully}. Features associated with the MPNN-, attention-, and CSGG-based approach are given in \cite{liu2024repsgg} and extended with inference methods summarized in 	\cref{tab:features}.


\subsection{Tools for the genereration of scene graphs}

Different tools have been developed to perform the intermediate operations required to generate scene graphs from sensed data. An overview of these tools is provided in \cref{tab:scenegraph_technologies}. YOLO (You Only Look Once) and SAM (Segment Anything Model) play major roles in object detection and segmentation tasks. SAM is a general-purpose model capable of pixel-level segmentation, whereas YOLO primarily provides bounding-box detection. YOLO is also optimized for low latency, making it suitable for real-time scenarios in dynamic, human-centered environments where rapid collision avoidance is critical. This enables faster collision handling while following human motion paths.

CLIP is widely used for processing semantic features due to its open‑vocabulary capabilities. It supports multi-modal classification at both pixel and bounding‑box levels. The shared semantic embedding space provided by CLIP enables a smooth transition from image-based perception to language‑driven understanding, for example through GPT based prompting.


\subsection{Operationalization  of scene graphs}
Scene graph creation  relies on the detection of objects on images and construction of a structured representation of these objects along with their semantic attributes, and relationships. This is done for different societal purposes \cite{amiri2022reasoning}. Included are  finding someone a place to sit in an indoor environment \cite{ravichandran2025safety}, assistance in cooking \cite{kwon2025large}, and  inspection missions in city-scale outdoor environments \cite{viswanathan2024xflie}. However, human activities are  characterized by a long-term horizon beyond an image.

 Long-term goals in  human-inhabited environments are handled in \cite{ravichandran2025safety} by federating multiple local scene graphs from previous pictures to yield a global scene graph.  In this work, we leverage a MLLM that exploits the reasoning history and its generalization capability to achieve long term reasoning. Such a contextual grounding is also enabled in \cite{kwon2025large} using the generative capability of an LLM in conjunction with preceding scenarios. We enlarge the scope of this scene-centered contextualization. Our goal is to integrate the mobile robotic device in the collective intelligence loop, as advocated in \cref{introi}. Specifically, internal processes of the robot, such as its energy consumption and motion dynamics, are taken into account. This allows for the management of both the  planning and physical feasibility of tasks. Indeed,  spatial goals specified by a human teammate and requirements on the robot dynamics are considered in the communication, taking care to keep the vocabulary as accessible as possible. A human teammate can thereby ask  the robot whether it has localized the target object in its current mental model and can physically reach that object long before an  attempt to move to the object is started. Visual question-answering queries have been used in \cite{morales2025vqa} to discover relevant events for a safe navigation through a city  and then build a scene graph that structures gained information to assist people with low vision in urban traffics. The device  that supports this assistive capability is a glass \cite{morales2025vqa}. In this work, the mobile robot not only follows  but also informs and guides its teammate from behind with a $360^{\circ}$ perspective on the environment  while dodging obstacles.


\section{Section}
%\begin{itemize}
%\item{First short definition of scene graph}
%\item{Comparison with other scene graph generation tools and frameworks such as [Concept Graph, OpenIN, HOVSG, Graph2Nav, 3DGraphLLM, SG-Nav, Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution, VLMaps ]  (need mapping beforehand and/or need powerful and additional hardware for models such as Grounding DINO, CLIP, GLIP, SAM, BLIP-2 and even additional drones for mapping, which does not suit our case [real-time, dynamic and unseen environment, reactive, portable/mobile, cost-effective or free/local models, audio interaction, suitable outdoors]). Nearly all the frameworks are used indoors.}
%\item{Most similar:  }
%\item{Can an Embodied Agent Find Your –Cat-shaped Mug? LLM-Based Zero-Shot Object Navigation (uses depth information, but needs mapping)}
%\item{VISIONGPT (YOLO World, ChatGPT, voice input and feedback)}
%\item {Cobots \& Service Robots \& Society 5.0/6.0}
%\end{itemize}

A scene graph (SG) serves to represent a scene with objects,
their attributes, and relationships between other objects in a structured way.
Scene graph generation (SGG) can be based on images, text, and videos.
Such a hierarchical graph data structure consists of nodes (O) and edges (E). Nodes (O) are objects such as persons, animals, places or things (bicycle, car, etc.), but also parts of other objects (e.g., limbs such as a person's arms). Each object node is usually equipped with attributes that describe the state of the object (e.g., colors, size, or pose). The edges (E) represent the relationships between the object pairs. These connections between objects describe actions (man holds cup) or positions (man stands in front of a bench). Typically, such a relationship is expressed as a
$\langle \text{subject} \;-\; \text{predicate} \;-\; \text{object} \rangle$ triple. 
An SG can be defined as a tuple:

\begin{equation}
	\label{eq:SG}
	SG = (\mathcal{O}, R, E)
\end{equation}

\noindent Where: 

\begin{itemize}
	\item $\mathcal{O} = \{o_1, \ldots, o_n\}$ is the set of objects recognized in the images, 
	where $n$ is the number of objects. Each object $o_i$ can also be described as a tuple: 
	\begin{equation}
		\label{eq:o_i}
		o_i = (c_i, A_i)
	\end{equation}
	where $c_i$ denotes the category (e.g., person, soccer stadium, animal) and 
	$A_i$ denotes the attributes (e.g., shape, color, pose) of the object.
	
	\item $R$ stands for the set of relationships between the nodes (objects).
	\item $E \subseteq \mathcal{O} \times R \times \mathcal{O}$ represents
	the edges between the object instance nodes 
	and the relationship nodes. The edge set $E$ consists of exactly those triples that are considered 
	relationships between the object pairs.
\end{itemize}
	
\noindent Using the following formulation, scene graphs can be expressed from an image $I$ using SGG methods:
	\begin{equation}
		\label{eq:SGG}
		SG(\mathcal{O}, R, E) = SGG(I)
	\end{equation}
	
\noindent For SGG, diverse models and technologies are employed across robotics, computer vision, autonomous navigation, and human-machine interaction. These are summarized in Table \ref{tab:scenegraph_technologies}. 
Due to the steadily increasing availability and improvements of Foundation Models, there has been a significant shift from class limited object recognition to universal segmentation models (e.g., Segment Anything Model, SAM) and open-vocabulary encoders (e.g., Contrastive Language–Image Pre-training, CLIP) that provide pixel-accurate masks and semantic embeddings, respectively. This allows modern Vision Language Models (VLMs) to recognize not only object classes but also their semantic states and complex relationships. Frameworks like ConceptGraphs and 3DGraphLLM use this information to construct scene graphs that are interpretable for LLMs and ground these objects in 3D space by integrating geometric metadata from sensors such as LiDAR or depth cameras. Approaches such as VLMaps and Graph2Nav focus on directly translating these spatial relations into executable navigation commands. 
Some of the frameworks presented require prior mapping or 3D reconstruction \cite{3dgraphllm}, \cite{hovsg}, \cite{openin}, \cite{interaction}, while others are designed for use in unknown environments and create a map in real time to navigate \cite{conceptgraphs}, \cite{graph2nav},\cite{sgnav},\cite{lgx}, \cite{vlmaps}. However, these require powerful hardware and various sensors.    





\section{Methodology}
This section presents the developed methods and their implementation. 

\subsection{Overview of Features and Components}
Our framework, coined pAIrSEEption, is a multimodal voice assistance system for environmental detection and navigation with a responsive asynchronous graphical user interface (GUI) based on PySide6 (see Fig. \ref{framework_architecture}), which serves as the interface between humans and the machine. Fig.\ref{overview_framework} illustrates the features of the user interface. The GUI can be installed across various platforms. For instance, the interface was installed on the Jetson AGX Orin (32GB version), an edge device and on a standard laptop with an integrated Nvidia graphics card. Subsequently, a ZED 2i stereo camera can be connected to these devices. The mobile Husky UGV robot \cite{husky} is utilized as the mounting platform for the components as shown in Fig.\ref{husky_robot}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=2.5in]{images/overview_eng.png}
	\caption{Overview of features and components of the pAIrSEEption framework.}
	\label{overview_framework}
\end{figure}

\begin{figure}[!b]
\centering
\includegraphics[width=2.5in]{images/husky_zed.jpg}
\caption{Husky Robot as a mounting platform for the ZED 2i camera.}
\label{husky_robot}
\end{figure}

The GUI acts as a visualization and control center for the user. All data streams from the various components are bundled here. The interface displays the following visual information: 

\begin{itemize}
  \item{A live, annotated video stream of the stereo camera from the perception data pipeline.}
  \item{System performance statistics, such as frames per second (FPS) and the percentage utilization of the central processing unit (CPU), graphics processing unit (GPU), video random access memory (VRAM), and random-access memory (RAM).}
  \item{Status and system messages for active and inactive processes}
  \item{Transcribed user voice commands}
  \item{Scene descriptions and casual chat as text generated by the LLM, which are created using object data and images from the current scene}
  \item{Navigation control overview to select a recognized object to navigate to and/or to follow}
  \item{In addition, internal robot data such as motor temperatures can be queried via voice input and are displayed in the GUI}
\end{itemize}

All data and information displayed on the interface, as well as user (voice) input, can be saved locally in common file formats via the GUI, facilitating any necessary fine-tuning or training of the models (YOLO and MLLM) in the future.

\subsection{Setup and Applications}
The interface provides a selection of different versions and sizes of YOLO and YOLOE (open prompt) models for object detection and segmentation as well as MLLMs for scene interpretation, which can be expanded as needed.  In addition, specific YOLO settings can be configured, such as showing and hiding bounding boxes, classification, and confidence. With an additional confidence slide bar, objects that are not well recognized can be filtered, which are below the set threshold. The dataflow can be divided into mainly five layers (Fig.\ref{framework_architecture}):
\begin{enumerate}
	\item Voice (Speech-To-Text, STT) or text input via the graphical user interface (scene description, navigation goal, robot status or casual chat). 
	\item Object detection with distance. If casual conversation with the MLLM is initiated then this step is skipped. 
	\item Preprocessing via rule-based intent analyzer depending on the input. This step could potentially be replaced with a small LLM for orchestrating the tasks, but a rule-based approach offers lower latency and is more ressource efficient. In dynamic and critical situations, e.g., in road traffic, fast processing is a top priority.
	\item Depending on the intention, the response is output via MLLM or rule-based text (preconfigured response) if the situation requires it. In any case, the implemented text-to-speech system (TTS) reads out the answer. Complex scene descriptions or casual conversations are forwarded to MLLM, while the robot's status and navigation goals (following an object or simply driving to an object) are rule-based processes.    
	\item When navigation or system status is requested, the system writes or subscribes to ROS2 topics to control the robot or query its status.
\end{enumerate}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{images/framework_architecture.pdf}
	\caption{Overview of the system architecture.}
	\label{framework_architecture}
\end{figure*}

Efficient YOLO-based object detection serves three purposes in the system. First, YOLO provides real-time 2D detections that are fused with the ZED 2i depth measurements, leveraging the ZED SDK, to estimate spatial object data (e.g., distance and 3D localization, see Fig. \ref{framework_architecture}), which are then formatted (Fig. \ref{transformation_template}) and passed to the MLLM as part of the grounded input prompt. Second, although modern MLLMs/VLMs can also produce 2D bounding boxes, YOLO is used as a lightweight preliminary detection layer to reduce MLLM workload and reduce inference latency.

%[The ZED SDK provides high-resolution stereo images and a depth map, which shows the distance of each pixel in the image. It also offers position tracking and a velocity estimation of the objects via the Python API, pyzed. This API can be used to configure camera parameters, such as resolution, FPS, depth mode, and depth measurement units. The resolution is set to 1280 x 720 pixels by default, with 60 FPS and performance mode for depth detection enabled. The unit of measurement is set to meters. The origin of the stereo camera is set to the coordinates ($x = 0$, $y = 0$, $z = 0$). For compatibility reasons, the camera's 3D coordinate system has been set to match the one used in Robot Operating System 2 (ROS2), a middleware framework for robots, since the mobile robot is based on it.]%

Third, it is used to navigate and follow a recognized object. To this end, the distance from each detected object to the camera and the pairwise inter-object distances within a frame are computed using the 3D Euclidean distance. The selection of the target object is initiated through its unique identification (ID), which is facilitated by either vocal input or interface selection. 


In pAIrSEEption, the objects \( \mathcal{O} = \{o_1, \ldots, o_n\} \) in the scene are schematically defined as follows (for example, a recognized person standing still):

%\[
%o_1 = \Bigl(\text{person}, \ 
%\left\{
%\begin{aligned}
%	& \text{ID} = 1, \\
%	& \text{confidence value} = 0.95, \\
%	& \text{3D position} = (x, y, z), \\
%	& \text{3D speed} = (v_x = 0, v_y = 0, v_z = 0), \\
%	& \text{speed classification} = \text{static}
%\end{aligned}
%\right\}
%\Bigr)
%\]


\begin{equation}
	\begin{aligned}
		o_1 = \Bigl(\text{person},\ 
		\left\{
		\begin{aligned}
			& \text{ID} = 1, \\
			& \text{confidence value} = 0.95, \\
			& \text{3D position} = (x, y, z), \\
			& \text{3D speed} = (v_x = 0, v_y = 0, v_z = 0), \\
			& \text{speed classification} = \text{static}
		\end{aligned}
		\right\}
		\Bigr)
	\end{aligned}
	\label{eq:o1}
\end{equation}


Where the object $o_1$ receives the attributes ID, confidence value, 3D positions, 3D velocity, and the classification \textit{static} if the velocity is zero. Thus, \textit{static} is an attribute derived from velocity, and the attribute set \textit{A} (see (\ref{eq:o_i})) is expanded with this additional information. The object data is expressed in the depth camera reference frame, i.e., the estimated 3D position ($x$, $y$, $z$) and the velocity ($v_x$, $v_y$, $v_z$) are relative to the camera.
The GUI offers various selections of MLLMs. These can be divided into two categories: 


The first category is local and offline models from Ollama such as Gemma 3 or Qwen 2.5 VL, hosted on consumer PCs 
in the laboratory or on an edge device like the Jetson AGX Orin attached to the mobile Husky Robot. Gemma 3 supports multimodale input with a very large context window and offers features that can improve attention to small details in images .Qwen 2.5 VL is often preferred for accurate, consistent captions—especially when text in the image (OCR). Both run locally with many quantization options and are capable of multiple languages \cite{gemma3} \cite{qwen25vl}. This solution offers the advantages of control, versioning, and data protection, but requires suitable hardware. Quantized and specialized or trainable models can be used for weaker hardware and a stable internet connection is not necessary. The second category is online, cloud hosted models via the OpenRouter API. With a single API key, it can connect to many different models from OpenAI, Google, Meta, Anthropic, etc. Most of these are subject to a fee per query and have a request limit. 
Nevertheless, inference takes place on the servers of the respective providers, meaning that significantly less powerful hardware (e.g., GPU) is required. In this case, factors such as server availability and data security are beyond the user's control. In potentially unstructured areas, connection problems may occur. For these reasons, both variants were implemented, which can be selected as needed and depending on internet availability and costs (resource costs and monetary costs).

The experiments (in section \ref{sec:experiments}) entail the implementation of accessible MLLMs on a local network server and the incorporation of the Jetson AGX Orin (32 GB version) into the mobile robot. The Jetson AGX Orin serves as the onboard PC, facilitating the comparison of onboard LLMs with fixed LLM models operating on more advanced hardware. Fig. \ref{client_server} depicts the implemented data pipeline, where the Jetson acts  as the client. The Jetson is used here as a representative PC on which the GUI application runs. However, we also show that even consumer notebooks can be used as a client. Due to the computing capacity of the Jetson Orin (32 GB or 64 GB), it is also possible to use it as a server for smaller local MLLMs.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=2.5in]{images/jetson_client_cropped.pdf}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{client_server}
\end{figure}

If a request for a description of the scene is initiated, the Jetson or laptop transmits an image of the current scene to the Ollama server—or, depending on the selection, also to OpenRouter—with the previously merged object data as a prompt to the server for interpretation and evaluation.
The following text-prompt instructs the MLLM on processing data. The goal is to enable safe navigation for a visually impaired person: 


%\begin{figure}[!htpb]
%	\centering
%	\includegraphics[width=2.5in]{images/grey_textbox.pdf}
%	\caption{Input prompt for the LLM to process with object data. Translated from German to English.}
%	\label{input_prompt}
%\end{figure}

%\begin{figure}[!htpb]
%	\centering
%	% define grey (5% black, 95% white)
%	\fcolorbox{black}{black!5}{%
%		\begin{minipage}{0.92\columnwidth} 
%			\setlength{\parskip}{0.5em} % paragraph distance
%			
%			``Describe the image briefly and precisely using the object data provided.
%			
%			\{object\_description\}
%			
%			Please describe what can be seen in the image,\\ 
%			where important objects are located,\\ 
%			and provide information about possible obstacles.''
%		\end{minipage}%
%	}
%	\caption{Input prompt for the LLM to process with object data (translated from German to English).}
%	\label{fig:llm_prompt}
%\end{figure}

%without fig. caption
%\vspace{1em} 
\noindent
\begin{center} 
	\fcolorbox{black}{black!5}{%
		\begin{minipage}{0.92\columnwidth}
			\setlength{\parskip}{0.5em}
			\ttfamily
			
			Describe the image briefly and precisely using the object data provided.
			
			\{object\_description\}
			
			Describe what can be seen in the image,
			where important objects are located,
			and provide information about possible obstacles.
		\end{minipage}%
	}
\end{center}
%\vspace{1em} 

The object data, defined as \textit{object\_description} is provided in the prompt and has been translated into concise bullet points during the preprocessing stage. Initially, a template for the raw data was employed. Fig. \ref{transformation_template} presents a schematic representation of the result of this transformation using the template. By combining the image data with the readable object data using the MLLM, an understandable 3D scene is described, which is accessible to humans.
The result of the scene interpretation is displayed as text in the output window of the GUI and can optionally be played back via an audio file generated by activating the text-to-speech function. This could be used, for example, to help a blind or visually impaired person navigate. The result of this semantic scene interpretation is shown in section \ref{subsec:sceneunderstanding}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\columnwidth]{images/transformation_template.png}
	\caption{Data preprocessing: Transformation of object data into a readable description for prompt input}
	\label{transformation_template}
\end{figure}


%\begin{figure}[htpb]
%	\centering
%	\includegraphics[width=8.7cm]{images/yolo_timeline_yolo11l_tensorRT.png}
%	\caption{Client-server architecture between application device and Ollama server.}
%	\label{yolo_timeline_tensorrt11}
%\end{figure}

\section{Evaluation and Results}
\label{sec:experiments}
This section presents hardware test results and benchmarks from experiments with YOLO models within pAIrSEEption and various inference times with MLLMs such as Gemma 3 and Qwen 2.5 VL until a scene description is generated. The second experiment shows results from a usability evaluation of the framework with a focus on human robot interaction and object following.

\subsection{Performance Analysis of YOLO Models}
Fig. \ref{fig:yolo_fps} and Fig. \ref{fig:yolo_gpu} show the average FPS and GPU usage of a variety of popular YOLO models of different sizes that are integrated into pAIrSEEption, respectively. The FPS value is an important measure for evaluating the perception module in the overall system and affects the response time, safety, precision of navigation, and tracking. The framework offers easy integration into the system by storing a PT, ONNX, or engine YOLO model in the same folder as the pAIrSEEption application. These models also include engine models that have been optimized with TensorRT (to 16-Bit Floating Point) on a consumer laptop (Nvidia RTX A2000 graphics card with 8 GB VRAM, 32 GB RAM and i7-13800H processor) to demonstrate accessibility.
A confidence threshold of 40\% was selected on the GUI. Data collection took place in a laboratory setting. The hardware performance data of the different YOLO models was recorded on the GUI over a set number of frames of around 700 to 1750 frames with varying activity within the scene (static and moving objects, while moving objects are counted as active). 
The data recording is part of the GUI, and the diagrams were created from this data. The results show that The yolo11n model achieves the best value in the average FPS evaluation with just under 40 FPS, which was to be expected as this model is the smallest in this test series.
The large yolo11l\_tensorRT is at a similar level and shows that optimization with TensorRT achieves a significant improvement of approximately 5 FPS on average compared to the standard yolo11l PT model. When comparing the two optimized TensorRT models in the overall system, the YOLO 11 version is ahead of version 8 by around 5 FPS on average and with a  negligibly higher GPU consumption of 2\% to 3\%.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\columnwidth] {images/yolo_performance_fps.png}
	\caption{FPS analysis of YOLO models on the laptop.}
	\label{fig:yolo_fps}
\end{figure}

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=\columnwidth, trim={0 550 720 42}, clip]{images/yolo_performance_1_of_2.png}
%	\caption{ FPS Analysis of YOLO Models on the laptop.}
%	\label{fig:yolo_fps}
%\end{figure}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\columnwidth] {images/yolo_performance_gpu.png}
	\caption{GPU usage analysis of YOLO Models on the laptop.}
	\label{fig:yolo_gpu}
\end{figure}


\subsection{Performance of MLLMs and Showcase of Scene Understanding}
\label{subsec:sceneunderstanding}
Fig. \ref{timing_breakdown} shows the duration of various Gemma 3 and Qwen 2.5 VL models to generate and describe a current scene with object data. Ollama is used to determine this  metric and record it in measurement logs. These logs consist of several parts. They contain metadata such as timestamps, an ID (for unique assignment of images and audio files to an analysis log), and the model used for scene interpretation. In addition, the number of input and output tokens is determined. In this test, the same scene was always tested with different models. 
The duration from when the user initiates a request for the scene description to the output of the response is broken down into three parts: the model load time, the input processing (both on the server as shown in Fig. \ref{client_server}) and the output generation sent from the server to the consumer laptop where pAIrSEEption runs. 

[...]

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\columnwidth] {images/lab.jpg}
	\caption{GPU usage analysis of YOLO Models on the laptop.}
	\label{mllm_scene_lab}
\end{figure}

\begin{center} 
	\fcolorbox{black}{black!5}{%
		\begin{minipage}{0.92\columnwidth}
			\setlength{\parskip}{0.5em}
			\ttfamily
			
		You are in a room, presumably an office.
		Directly in front of you, about 1.8 meters away and slightly to the left, is a screen. To your right, about 2.5 meters away, is a chair. To your left, about 3.7 meters away, is another chair.
		A little further ahead, about 4 meters away and to your left, is another
		screen. To your right, about 5 meters away, is another chair. Be aware of the chairs and screens as potential obstacles.
		The floor appears to be level.
		\end{minipage}%
	}
\end{center}

[...]

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/timing_breakdown.png}
	\caption{Timing breakdown of Gemma 3 and Qwen 2.5 VL models based on size. The suffix "Husky" indicates that the model has been modified using modelfiles and adapted to our application. Abbreviations: "G3" = Geemma 3, "it qat" = instruction-tuned and Quantization-Aware Training, "Q4" = 4-Bit quantization.}
	\label{timing_breakdown}
\end{figure}

\subsection{Usability Study}
%\begin{table*}[!ht]
%	\centering
%	\caption{Quantitative Results of the Usability Evaluation ($N=7$) featuring Mean Score $\bar{x}$ and Sample Standard Deviation $s$. Scoring: 5 = highest/best, 1 = lowest/worst.}
%	\label{tab:user_study_results}
%	\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
%		\hline
%		\textbf{N=7} & \textbf{Q1} & \textbf{Q2} & \textbf{Q3*} & \textbf{Q4} & \textbf{Q5} & \textbf{Q6*} & \textbf{Q7} & \textbf{Q8} & \textbf{Q9} \\ \hline
%		P1 & 4 & 4 & 3 & 5 & 4 & 1 & 4 & 5 & 4 \\ \hline
%		P2 & 4 & 4 & 3 & 5 & 5 & 1 & 4 & 5 & 4 \\ \hline
%		P3 & 5 & 5 & 1 & 4 & 5 & 2 & 5 & 5 & 5 \\ \hline
%		P4 & 5 & 5 & 1 & 5 & 5 & 2 & 5 & 5 & 5 \\ \hline
%		P5 & 5 & 5 & $-$ & 4 & 5 & 1 & 5 & 5 & 5 \\ \hline
%		P6 & 5 & 5 & 1 & 5 & 5 & 1 & 4 & 4 & 5 \\ \hline
%		P7 & 5 & 4 & 2 & 5 & 4 & 3 & 4 & 4 & 3 \\ \hline \hline
%		\textbf{Mean} $\bar{x}$ & \textbf{4.71} & \textbf{4.57} & \textbf{1.83} & \textbf{4.71} & \textbf{4.71} & \textbf{1.57} & \textbf{4.43} & \textbf{4.71} & \textbf{4.43} \\ \hline
%		\textbf{SD} $s$ & \textbf{0.45} & \textbf{0.49} & \textbf{0.90} & \textbf{0.45} & \textbf{0.45} & \textbf{0.73} & \textbf{0.49} & \textbf{0.45} & \textbf{0.73} \\ \hline
%	\end{tabular}
%	\vspace{1ex}
%	\begin{flushleft}
%		\small *Negative questions: lower values indicate better performance (fewer errors/issues).
%	\end{flushleft}
%\end{table*}

%\subsubsection{Survey Design}
%\begin{table}[!ht]
%	\centering
%	\caption{Survey Questionnaire Items and Scales}
%	\label{tab:questionnaire}
%	\footnotesize 
%	\begin{tabular}{|l|p{4.5cm}|l|}
%		\hline
%		\textbf{ID} & \textbf{Question (Translated from German)} & \textbf{Scale (1--5)} \\ \hline
%		\multicolumn{3}{|l|}{\textit{Part 1: Voice Control}} \\ \hline
%		Q1 & How easy was it to give voice commands? & Very hard -- Very easy \\ \hline
%		Q2 & Did the robot reliably recognize your commands? & Never -- Always \\ \hline
%		Q3* & Did you have to repeat commands? & Never -- Always \\ \hline
%		\multicolumn{3}{|l|}{\textit{Part 2: Object Tracking}} \\ \hline
%		Q4 & Did the robot identify the correct object? & Never -- Always \\ \hline
%		Q5 & How fast did the robot react to your command? & Very slow -- Very fast \\ \hline
%		Q6* & Did the robot lose the object? & Never -- Always \\ \hline
%		\multicolumn{3}{|l|}{\textit{Part 3: User Experience}} \\ \hline
%		Q7 & How easy was the overall operation? & Very hard -- Very easy \\ \hline
%		Q8 & Did you feel safe while the robot followed? & Not at all -- Fully \\ \hline
%		Q9 & Would you use this in real applications? & No -- Yes \\ \hline
%	\end{tabular}
%\end{table}

\subsubsection{Experimental Setup}
Participants (N = 7) from various technical backgrounds (Engineering Bachelor and Master levels) were asked to interact  with the mobile Husky robot (indoors) using voice commands in German (e.g., "Folge Person 1" - "Follow Person 1") after a brief introduction of the GUI and the system. After voice input, the robot should then pursue the corresponding target over a random distance determined by the student. Voice control can also be used to stop or start a different navigation. The robot should provide audio feedback to the user about its actions. After the interaction, the students were asked to anonymously evaluate their overall impression in a survey.

\subsubsection{Study Design}
To evaluate the usability of mainly the voice control and object tracking (following an object with the mobile robot), a survey consisting of nine Likert-scale items (with scores of 1 to 5) and three open questions was conducted. The items were structured as follows:

\begin{itemize}[leftmargin=*]
	\item \textbf{Voice Control:} 
	\begin{enumerate}[label=Q\arabic*:, series=survey, widest=Q12, leftmargin=*]
		\item Ease of giving commands. 
		\item Recognition reliability. 
		\item Necessity of repetitions (negative item*). 
	\end{enumerate}
	
	\item \textbf{Object Tracking:} 
	\begin{enumerate}[label=Q\arabic*:, resume=survey, widest=Q12, leftmargin=*]
		\item Correct object identification.
		\item System response time. 
		\item Frequency of losing the target (negative item*). 
	\end{enumerate}
	
	\item \textbf{User Experience:} 
	\begin{enumerate}[label=Q\arabic*:, resume=survey, widest=Q12, leftmargin=*]
		\item Overall ease of use. 
		\item Perceived safety during operation. 
		\item Intention to use in real-world applications. 
	\end{enumerate}
	
	\item \textbf{Qualitative Feedback (Open-ended):} 
	\begin{enumerate}[label=Q\arabic*:, resume=survey, widest=Q12, leftmargin=*]
		\item Positive aspects of the system operation. 
		\item Suggestions for improving user comfort.
		\item Proposed additional functions or commands. 
	\end{enumerate}
\end{itemize}

The questions were rated by participants on a scale of 1 to 5, with 1 being the lowest/worst rating and 5 being the highest/best rating. For questions Q3 and Q6, negative items were reverse-coded for statistical analysis. For every question (Q1 to Q9) a mean $\bar{x}$ and a sample standard deviation $s$ was computed to make a quantitative statement. Since participant P5 skipped question Q3, n = 6 was calculated for this question.

\subsubsection{Results and Discussion}

\begin{table}[htpb]
	\centering
	\caption{Quantitative Results of the Usability Evaluation ($N=7$) featuring Mean Score $\bar{x}$ and Sample Standard Deviation $s$. Scoring: 5 = highest/best, 1 = lowest/worst.
	*Negative questions: lower values indicate better performance.}
	\label{tab:user_study_results}
	\setlength{\tabcolsep}{3.5pt} 
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{Participant} & \textbf{Q1} & \textbf{Q2} & \textbf{Q3*} & \textbf{Q4} & \textbf{Q5} & \textbf{Q6*} & \textbf{Q7} & \textbf{Q8} & \textbf{Q9} \\ \hline
		P1 & 4 & 4 & 3 & 5 & 4 & 1 & 4 & 5 & 4 \\ \hline
		P2 & 4 & 4 & 3 & 5 & 5 & 1 & 4 & 5 & 4 \\ \hline
		P3 & 5 & 5 & 1 & 4 & 5 & 2 & 5 & 5 & 5 \\ \hline
		P4 & 5 & 5 & 1 & 5 & 5 & 2 & 5 & 5 & 5 \\ \hline
		P5 & 5 & 5 & $-$ & 4 & 5 & 1 & 5 & 5 & 5 \\ \hline
		P6 & 5 & 5 & 1 & 5 & 5 & 1 & 4 & 4 & 5 \\ \hline
		P7 & 5 & 4 & 2 & 5 & 4 & 3 & 4 & 4 & 3 \\ \hline \hline
		\textbf{Mean} $\bar{x}$ & \textbf{4.71} & \textbf{4.57} & \textbf{1.83} & \textbf{4.71} & \textbf{4.71} & \textbf{1.57} & \textbf{4.43} & \textbf{4.71} & \textbf{4.43} \\ \hline
		\textbf{SD} $s$ & \textbf{0.45} & \textbf{0.49} & \textbf{0.90} & \textbf{0.45} & \textbf{0.45} & \textbf{0.73} & \textbf{0.49} & \textbf{0.45} & \textbf{0.73} \\ \hline
	\end{tabular}
	\vspace{1ex}
\end{table}

The quantitive results in Table \ref{tab:user_study_results} indicate a high level of user satisfaction across all categories. Particular emphasis was placed on the ease of use of voice commands, the subsequent correct object identification by the system, the system response time, and the perceived safety during robot operation (Q1, Q4, Q5 and Q8: $\bar{x}$ = 4.71 of 5 and in each case $s$ = 0.45). Furthermore, the reliability of object recognition was rated as very good (Q2: $\bar{x}$ = 4.57 and Q3: $\bar{x}$ = 1.83) and the overall operation of robot and GUI (Q7: $\bar{x}$ = 4.43) was also rated very highly. The participants are also willing to use such a system in real-world applications (Q9: $\bar{x}$ = 4.43). 
While Q6: $\bar{x}$ = 1.57 further confirms that the tracking behavior was perceived as predictable and non-threatening. 
The low standard deviation $s$ across most items indicates a high consensus among participants.
It should be noted, however, that the participants all come from the field of engineering, so that an intrinsic interest in technology must be assumed. The study is also limited by the small sample size ($N=7$), which serves as a pilot evaluation.  
For further investigations, visually impaired individuals should be consulted, for example, to evaluate the system's scene description and determine in what ways it can be helpful for independent navigation. In addition, while the current object tracking performance was rated highly, future work should investigate more cluttered/unstructured environments where the probability of losing the target (Q6) might increase. 
The qualitative feedback (Q10 to Q12) reveal that participants particularly emphasized the precision and fast response time of the system. They also praised the easy-to-understand and clear interface and the constant distance maintenance.
Potential for improvement was seen primarily in the robot driving around corners (slow turns). Suggestions were also made for optimizing re-identification (Re-ID) of the object, improving obstacle detection, and providing more detailed instructions for voice commands.
Additional functions suggested included a reverse gear, a voice command for a 180° turn, and the integration of an additional robot arm.

Overall, the results suggest that our system can effectively lower the entry barrier for robot interaction and shows great potential for service robotics in domestic or industrial settings.

\section{Conclusion}
The conclusion goes here.


\section*{Acknowledgments}
We would like to thank Mr. S. Azizpour for his support in conducting the usability study. We would also like to thank all participants for their time and feedback.


%{\appendix[Proof of the Zonklar Equations]
%Use $\backslash${\tt{appendix}} if you have a single appendix:
%Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
%If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
%You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
% starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}



%\section{References Section}
%You can use a bibliography generated by BibTeX as a .bbl file.
%BibTeX documentation can be easily obtained at:
%http://mirror.ctan.org/biblio/bibtex/contrib/doc/
%The IEEEtran BibTeX style support page is:
%http://www.michaelshell.org/tex/ieeetran/bibtex/

% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
%\section{Simple References}
%You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
%(used to reserve space for the reference number labels box).

\begin{thebibliography}{1}
	\bibliographystyle{IEEEtran}
\bibitem{obrenovic2025generative}Obrenovic, B., Gu, X., Wang, G., Godinic, D. \& Jakhongirov, I. Generative AI and human–robot interaction: implications and future agenda for business, society and ethics. {\em AI \& Society}. \textbf{40}, 677-690 (2025)

\bibitem{kaigom2023metarobotics}Kaigom, E. Metarobotics for industry and society: Vision, technologies, and opportunities. {\em IEEE Transactions On Industrial Informatics}. \textbf{20}, 5725-5736 (2023)

	\bibitem{singh2024pushing}
	Singh, C. and Jayadas, A. Pushing Shopping Cart: An Intentional Exercise for Older Adults. {\em Activities, Adaptation \& Aging}. \textbf{48}, 670-689 (2024)



	\bibitem{gona2024intelligent}Gona, S. \& Harish, C. Intelligent mobility planning for a cost-effective object follower mobile robotic system with obstacle avoidance using robot vision and deep learning. {\em Evolutionary Intelligence}. \textbf{17}, 1279-1293 (2024)

\bibitem{yalcinkaya2024towards}Yalcinkaya, B., Couceiro, M., Pina, L., Soares, S., Valente, A. \& Remondino, F. Towards Enhanced Human Activity Recognition for Real-World Human-Robot Collaboration. {\em 2024 IEEE International Conference On Robotics And Automation (ICRA)}. pp. 7909-7915 (2024)

\bibitem{rana2023sayplan}Rana, K., Haviland, J., Garg, S., Abou-Chakra, J., Reid, I. \& Suenderhauf, N. Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. {\em ArXiv Preprint ArXiv:2307.06135}. (2023)\\

\bibitem{im2024egtr}Im, J., Nam, J., Park, N., Lee, H. \& Park, S. Egtr: Extracting graph from transformer for scene graph generation. {\em Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition}. pp. 24229-24238 (2024)

\bibitem{hao2025bctr}Hao, P., Wang, W., Wang, X., Jiang, Y., Jia, H., Cui, S., Wei, J. \& Hao, X. BCTR: bidirectional conditioning transformer for scene graph generation. {\em Information Fusion}. pp. 103260 (2025)

\bibitem{liu2021fully}Liu, H., Yan, N., Mortazavi, M. \& Bhanu, B. Fully convolutional scene graph generation. {\em Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition}. pp. 11546-11556 (2021)

\bibitem{liu2024repsgg}Liu, H. \& Bhanu, B. Repsgg: Novel representations of entities and relationships for scene graph generation. {\em IEEE Transactions On Pattern Analysis And Machine Intelligence}. \textbf{46}, 8018-8035 (2024)

\bibitem{newell2017pixels}Newell, A. \& Deng, J. Pixels to graphs by associative embedding. {\em Advances In Neural Information Processing Systems}. \textbf{30} (2017)

\bibitem{amiri2022reasoning}Amiri, S., Chandan, K. \& Zhang, S. Reasoning with scene graphs for robot planning under partial observability. {\em IEEE Robotics And Automation Letters}. \textbf{7}, 5560-5567 (2022)

\bibitem{ravichandran2025safety}Ravichandran, Z., Robey, A., Kumar, V., Pappas, G. \& Hassani, H. Safety Guardrails for LLM-Enabled Robots. {\em ArXiv Preprint ArXiv:2503.07885}. (2025)

\bibitem{kwon2025large}Kwon, S., Park, J., Jang, H., Roh, C. \& Chang, D. Large Language Model Based Autonomous Task Planning for Abstract Commands. {\em 2025 IEEE International Conference On Robotics And Automation (ICRA)}. pp. 11010-11016 (2025)



\bibitem{viswanathan2024xflie}Viswanathan, V., Saucedo, M., Satpute, S., Kanellakis, C. \& Nikolakopoulos, G. xFLIE: Leveraging Actionable Hierarchical Scene Representations for Autonomous Semantic-Aware Inspection Missions. {\em ArXiv Preprint ArXiv:2412.19571}. (2026)

\bibitem{morales2025vqa}Morales, J., Gebregziabher, B., Cabañeros, A. \& Sanchez-Riera, J. VQA-Driven Event Maps for Assistive Navigation for People with Low Vision in Urban Environments. {\em 2025 IEEE International Conference On Robotics And Automation (ICRA)}. pp. 12458-12464 (2025)

\bibitem{kim2025semantic}Kim, H. \& Ko, B. Semantic scene graph generation based on an edge dual scene graph and message passing neural network. {\em Image And Vision Computing}. pp. 105572 (2025)\\

\bibitem{su2024adopting}Su, P. \& Chen, D. Adopting graph neural networks to analyze human–object interactions for inferring activities of daily living. {\em Sensors}. \textbf{24}, 2567 (2024)

\bibitem{husky}"Husky A200 User Manual," Clearpath, https://docs.clearpathrobotics.com/docs\_robots/outdoor\_robots/husky/\\a200/user\_manual\_husky (accessed Feb. 18, 2026).

	\bibitem{conceptgraphs}
	Q. Gu et al., "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5021-5028, doi: 10.1109/ICRA57147.2024.10610243.
	
	\bibitem{3dgraphllm}
	Tatiana Zemskova and Dmitry Yudin, "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding," 2025, https://doi.org/10.48550/arXiv.2412.18450
	
	\bibitem{graph2nav}
	T. Shan, A. Rajvanshi, N. Mithun and H. -P. Chiu, "Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation," 2025 IEEE International Conference on Robotics and Automation (ICRA), Atlanta, GA, USA, 2025, pp. 1646-1652, doi: 10.1109/ICRA55743.2025.11128782. 
	
	\bibitem{hovsg}
	Robotics: Science and Systems 2024, Delft, Netherlands, July 15-July 19, 2024, 
	https://doi.org/10.15607/RSS.2024.XX.077
	
	\bibitem{openin}
	Y. Tang et al., "OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic Environments," in IEEE Robotics and Automation Letters, vol. 10, no. 9, pp. 9256-9263, Sept. 2025, doi: 10.1109/LRA.2025.3592071.
	
	\bibitem{sgnav}
	H. Yin et al., "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation,"
	https://doi.org/10.48550/arXiv.2410.08189
	
	\bibitem{interaction}
	Q. Li, X. Zhang, C. Chen, H. Zhao and J. Niu, "Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution," 2025 IEEE International Conference on Robotics and Automation (ICRA), Atlanta, GA, USA, 2025, pp. 11933-11939, doi: 10.1109/ICRA55743.2025.11128194.
	
	\bibitem{visiongpt}
	H. Wang et al., "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation,"
	https://doi.org/10.48550/arXiv.2403.12415
	
	\bibitem{lgx}
	V. S. Dorbala, J. F. Mullen and D. Manocha, "Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation," in IEEE Robotics and Automation Letters, vol. 9, no. 5, pp. 4083-4090, May 2024, doi: 10.1109/LRA.2023.3346800.
	
	\bibitem{vlmaps}
	C. Huang, O. Mees, A. Zeng and W. Burgard, "Visual Language Maps for Robot Navigation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 10608-10615, doi: 10.1109/ICRA48891.2023.10160969.
	
	\bibitem{gemma3}
	Gemma Team et al., "Gemma 3 Technical Report," arXiv:2503.19786, 2025.
	[Online]. Available: https://doi.org/10.48550/arXiv.2503.19786
	
	\bibitem{qwen25vl}
	S. Bai et al., “Qwen2.5-VL Technical Report,” arXiv:2502.13923, 2025.
	[Online]. Available: 
	https://doi.org/10.48550/arXiv.2502.13923
	
	
\end{thebibliography}


\newpage

\section{Biography Section}
If you have an EPS/PDF photo (graphicx package needed), extra braces are
needed around the contents of the optional argument to biography to prevent
the LaTeX parser from getting confused when it sees the complicated
$\backslash${\tt{includegraphics}} command within an optional argument. (You can create
your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
simpler here.)

\vspace{11pt}

\bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
	Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
	Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

\vspace{11pt}

\bf{If you will not include a photo:}\vspace{-33pt}
\begin{IEEEbiographynophoto}{John Doe}
	Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
\end{IEEEbiographynophoto}




\vfill

\end{document}


