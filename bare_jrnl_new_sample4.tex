\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{varioref}
\usepackage{cleveref}
%added xcolor for box color
\usepackage{xcolor}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}
	
	\title{pAIrSEEption: LLM- and YOLO-Driven Scene Graph for Inclusive Human-Robot  Teaming in the Society of the Future}
	
	\author{Trung Kien La and Eric Guiffo Kaigom
		% <-this % stops a space
		\thanks{Trung Kien La was with the Frankfurt Industrial Robotics and Digital Twin Laboratory (FriiDA) at the Frankfurt University of Applied Sciences, Hungener Str. 6 Building C, 60389 Frankfurt am Main, Germany (Orcid: 0009-0007-9239-3268).}% <-this % stops a space
		\thanks{Eric Guiffo Kaigom is with the Frankfurt Industrial Robotics and Digital Twin Laboratory (FriiDA) at the Frankfurt University of Applied Sciences, Hungener Str. 6 Building C, 60389 Frankfurt am Main, Germany (e-mail: kaigom@fra-uas.de).}}
	
	% The paper headers
	\markboth{Replace this line with your manuscript id number}%
	{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}
	
	%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
	% Remember, if you use this you must call \IEEEpubidadjcol in the second
	% column for its text to clear the IEEEpubid mark.
	
	\maketitle
	
	\begin{abstract}
		People regularly need to reach and move loads in daily life. Many essential activities in the society build on it.  
		However, even a short distance and an increased physical exertion make especially people with limited motor and visual abilities refrain from  these activities. Robots can help sustain self-determination and preserve health in this context. However,  robots are mostly developed for scripted motions and subject to programming complexity viewed as a barrier for their adoption. It remains hard to \textit{verbally ask} a robot to reach a previously unseen target object in its workspace or to follow a human operator.  
		Conversely, robots seldom share their current perception and reasoning about the sensed environment with humans, failing to engage humans in understanding, refining, and adapting their   expectations from a robotic assistance and guidance when approaching or carrying a payload. In this work, we describe the development of an infrastructure that combines an YOLO-driven semantic object detection and Large Language Models (LLMs) to yield a scene graph. The structured and meaningful representation of the operational space enables  a mobile payloaded robot to reach or track a designated target entity, such as a chair or a human. On the other hand, the robot communicates its perception of the environment along with feedback on task execution and its internals to the human operator using a natural language in real-time. Experiment results show that the extended robot intelligence simplifies the penetration of robots in societal applications, facilitates the inclusion  and fosters the self-determination of citizens in societal applications, paving the ground for an enhancement of the perceived usability of robot in the daily life in the society of the future. 
	\end{abstract}
	
	\begin{IEEEkeywords}
		Society of the Future, Intelligent Robot, Large Language Models, YOLO, Scene Graph,  Extended Robot Intelligence
	\end{IEEEkeywords}
	
	\section{Introduction}
	\IEEEPARstart{T}{he} society of the future will be enriched with emerging digital technologies that support the well-being of humans by enhancing    their inclusion and self-determination \cite{obrenovic2025generative}. 
	Learned collective intelligence driven by large language models (LLMs), such as GPT, LLaMA, Gemini and Claude, and generative AI tools like ChatGPT  are increasingly fostering inclusion. For instance, access to globally shared information in a desired spoken natural language is being more and more possible. On the other hand, mechanically assisted motor skills provided by e.g. wearable robots  for physically demanding tasks are witnessing notable advances. However, there are still   limitations when it comes to carry heavy payloads over a long distance without muscular or ergonomic risks for humans \cite{singh2024pushing}. Furthermore, the integration of  mechanical devices in the  collective intelligence loop  to support the self-determination of humans through natural  interactions with  robots has been overlooked thus far. Reasons span from the fundamental automation idea of robots to the lack of interfaces that bring them close to citizens in terms of communication and usability.
	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth]{pic/movingbox.png}
		\caption{Healthy payload handling without stress over a long distance using a  mobile robot with extended intelligence. The robot carries the payload, builds and shares a mental scene graph-based model to reason about its environment, autonomously avoids collisions, and  follows the human operator to the intended location while giving contextualized feedback in real-time.}
		\label{intro}
	\end{figure}
	
	While mobile robots are useful to carry a wide range of payloads without physical stress \cite{gona2024intelligent}, citizens struggle to ask them to move to a target location. Doing so requires a paradigm shift from programming to communication. A practical societal application is dynamically moving a heavy box from a place to another without workspace calibration, in which case the robot is steered to a desired location by following a human operator as shown in \cref{intro}. Observe that the operator  freely and comfortably moves around and the robot tracks the human path while reasoning to  avoid potential collisions. Nevertheless, robots fail to communicate their mental model, i.e., understanding of the operational environment, intended motion, and internal state, to humans in real-time. This bidirectional gap unfortunately prevents citizens to use a desired natural language  to engage in a service-oriented dialog with the robot to anticipate events, refine their expectations,  or adapt  their individual \textit{requests} (instead of hard-coded \textit{commands}) to the robot on the fly without writing any line of code.
	
	The immediate advantages of doing so are manifold for a human teammate. The communication with the robot can occur in  a desired language supported by an LLM. Cultural particularities can be conveyed more readily.  A technical  background is no longer necessary to prompt the robot to complete a task. People can translate complex tasks into  sequences of single spoken request primitives to achieve demanding objectives. For example, following a complex path can be broken down into  intermediate motions of the robot that aim to attain the vicinity of previously unseen objects that suddenly appear in the workspace of the moving robot  (see \cref{mental}). A walking human teammate  can act as a guide followed upon invitation by the robot that serves as a assistive device  carrying a payload to a desired location. Meanwhile, the robot can exploit its mental model of the environment to amend human guidance (e.g., in the case of an imminent collision) and give directive feedback about the environment to human. This shared autonomy democratizes  access to robots and fosters the  inclusion as well as the self-determination of human operators. 
	
	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth]{pic/mental.png}
		\caption{Semantic understanding of the \textit{indoor} operational environment by the robot to yield and update its mental model as a scene graph. Here, the robot follows a chair in motion designated by the human operator as target object using LLM. The visual perception is achieved through processing data from a depth camera and YOLO-based object recognition. A structured and meaningful representation of the 3D scene is then used to enable a safe robot navigation and give assistive as well as directive feedback to the human operator.}
		\label{mental}
	\end{figure}
	
	We describe the development of an infrastructure for code-free teaming between human and mobile robot in societal applications.  Included are indoor assistance, in which case the robot acts as commissioned  agent and follower for synergistic payload transport. Everyone can benefit from this functionality regardless of the technical background. Furthermore, the robot  pro-actively offers outdoor guidance that can be helpful for individuals with e.g. visual impairments. Our main contributions are as follows.% Multimodal Large Language Models (MLLMs) are leveraged to establish a bidirectional, natural-language communication between humans and robots. 
	\begin{itemize}
		\item We leverage YOLO to develop a dynamic scene graph that provides a structured and semantic representation of the current workspace as a mental model for mobile robots. 
		\item We combine the scene graph and an LLM-driven communication to help any citizen intuitively and safely navigate a payloaded mobile robot through its understood environment to the vicinity of any freely designated entity, such as a human or an object, even in motion.
		\item We demonstrate the usefulness of our extended robot intelligence for cooperative task execution based on a shared autonomy between human and robot in societal applications. 
	\end{itemize}
The remainder of this work is organized as follows.

	\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth]{pic/mental2.png}
	\caption{Semantic understanding of the \textit{outdoor} operational environment by the robot to construct and update its mental model of its workspace as a scene graph. Here, the robot not only follows the human teammate, but also give hints about surrounding objects using YOLO and LLM. Objects are e.g. obstacles or  useful items like a bicycle. Note that the robot is always aware of an estimation of the relative pose  (i.e., position and orientation) and velocity of objects. It uses this information for human guidance, reachability assessment upon LLM-driven request,  and collision avoidance purposes. Hence, the robot employs AI to perceive the scene (we coined it \textit{pAIrception}) for the humans.}
	\label{mental}
\end{figure}

\begin{table*}[!hb]
	\centering
	\caption{Technical Overview of Frameworks for 3D Scene Graph Generation, Perception, Navigation and Interfacing in Robotics}
	\label{tab:scenegraph_technologies}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{System/Framework} & \textbf{\begin{tabular}[c]{@{}l@{}}Detection \& Segmentation \\ (Nodes)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Semantic Features \& \\ Embeddings\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Relation Inference \& \\ Planning (Edges)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Alternatives \& \\ Additional Models \& \\ Misc. \end{tabular}}  \\ \hline
			\textbf{ConceptGraphs}\cite{conceptgraphs} & SAM & CLIP, Grounding DINO & GPT-4, LLaVA & RAM \\ \hline
			\textbf{3DGraphLLM}\cite{3dgraphllm} & Mask3D, OneFormer3D & DINOv2 (2D), Uni3D (3D) & VL-SAT, CLIP & LLaMA3-8B, Vicuna-1.5 \\ \hline
			\textbf{Graph2Nav}\cite{graph2nav} & PSGFormer, Mask2Former & Panoptic Labels & 2D-PSG Inference, GPT-3.5 & LIO-SAM, Whisper \\ \hline
			\textbf{HOV-SG}\cite{hovsg} & SAM & CLIP & Hierarchical Heuristics & GPT-3.5, GPT-4 \\ \hline
			\textbf{OpenIN}\cite{openin} & CropFormer & SBERT, CLIP & GPT-4o & Tokenize Anything \\ \hline
			\textbf{SG-Nav}\cite{sgnav} & SAM & VLMs, LLMs & LLaMA-7B, GPT-4, LLaVA & Fast Marching Method \\ \hline
			\textbf{Interaction Updates}\cite{interaction} & Existing 3D SG (e.g., Hydra) & Visual Transformer (ViT) & GPT-4, BLIP-2 & DBSCAN (Denoising) \\ \hline
			\textbf{VisionGPT}\cite{visiongpt} & YOLO-World & Proportional 2D Coordinates & GPT-3.5 Turbo, GPT-4 & H-Pattern Splitter \\ \hline
			\textbf{LGX}\cite{lgx} & GLIP, YOLO & BLIP (Captions) & GPT-3 & OWL-ViT, Detectron2 \\ \hline
			\textbf{VLMaps}\cite{vlmaps} & LSeg & CLIP & Codex (Code-writing LLM) & RTAB-Map \\ \hline
		\end{tabular}%
	}
\end{table*}

\section{Related Work}
\begin{itemize}
\item{First short definition of scene graph}
\item{Comparison with other scene graph generation tools and frameworks such as [Concept Graph, OpenIN, HOVSG, Graph2Nav, 3DGraphLLM, SG-Nav, Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution, VLMaps ]  (need mapping beforehand and/or need powerful and additional hardware for models such as Grounding DINO, CLIP, GLIP, SAM, BLIP-2 and even additional drones for mapping, which does not suit our case [real-time, dynamic and unseen environment, reactive, portable/mobile, cost-effective or free/local models, audio interaction, suitable outdoors]). Nearly all the frameworks are used indoors.}
\item{Most similar:  }
\item{Can an Embodied Agent Find Your –Cat-shaped Mug? LLM-Based Zero-Shot Object Navigation (uses depth information, but needs mapping)}
\item{VISIONGPT (YOLO World, ChatGPT, voice input and feedback)}
\item {Cobots \& Service Robots \& Society 5.0/6.0}
\end{itemize}

A scene graph (SG) serves to represent a scene with objects,
their attributes, and relationships between other objects in a structured way.
Scene graph generation (SGG) can be based on images, text, and videos.
Such a hierarchical graph data structure consists of nodes (O) and edges (E). Nodes (O) are objects such as persons, animals, places or things (bicycle, car, etc.), but also parts of other objects (e.g., limbs such as a person's arms). Each object node is usually equipped with attributes that describe the state of the object (e.g., colors, size, or pose). The edges (E) represent the relationships between the object pairs. These connections between objects describe actions (man holds cup) or positions (man stands in front of a bench). Typically, such a relationship is expressed as a
$\langle \text{subject} \;-\; \text{predicate} \;-\; \text{object} \rangle$ triple. [small SG image excerpt here]
An SG can be defined as a tuple:

\begin{equation}
	\label{eq:SG}
	SG = (\mathcal{O}, R, E)
\end{equation}

\noindent Where: 

\begin{itemize}
	\item $\mathcal{O} = \{o_1, \ldots, o_n\}$ is the set of objects recognized in the images, 
	where $n$ is the number of objects. Each object $o_i$ can also be described as a tuple: 
	\begin{equation}
		\label{eq:o_i}
		o_i = (c_i, A_i)
	\end{equation}
	where $c_i$ denotes the category (e.g., person, soccer stadium, animal) and 
	$A_i$ denotes the attributes (e.g., shape, color, pose) of the object.
	
	\item $R$ stands for the set of relationships between the nodes (objects).
	\item $E \subseteq \mathcal{O} \times R \times \mathcal{O}$ represents
	the edges between the object instance nodes 
	and the relationship nodes. The edge set $E$ consists of exactly those triples that are considered 
	relationships between the object pairs.
\end{itemize}
	
\noindent Using the following formulation, scene graphs can be expressed from an image $I$ using SGG methods:
	\begin{equation}
		\label{eq:SGG}
		SG(\mathcal{O}, R, E) = SGG(I)
	\end{equation}
	
\noindent For SGG, diverse models and technologies are employed across robotics, computer vision, autonomous navigation, and human-machine interaction. These are summarized in Table \ref{tab:scenegraph_technologies}. 
Due to the steadily increasing availability and improvements of Foundation Models, there has been a significant shift from class limited object recognition to universal segmentation models (e.g., Segment Anything Model, SAM) and open-vocabulary encoders (e.g., Contrastive Language–Image Pre-training, CLIP) that provide pixel-accurate masks and semantic embeddings, respectively. This allows modern Vision Language Models (VLMs) to recognize not only object classes but also their semantic states and complex relationships. Frameworks like ConceptGraphs and 3DGraphLLM use this information to construct scene graphs that are interpretable for LLMs and ground these objects in 3D space by integrating geometric metadata from sensors such as LiDAR or depth cameras. Approaches such as VLMaps and Graph2Nav focus on directly translating these spatial relations into executable navigation commands. 
Some of the frameworks presented require prior mapping or 3D reconstruction \cite{3dgraphllm}, \cite{hovsg}, \cite{openin}, \cite{interaction}, while others are designed for use in unknown environments and create a map in real time to navigate \cite{conceptgraphs}, \cite{graph2nav},\cite{sgnav},\cite{lgx}, \cite{vlmaps}. However, these require powerful hardware and various sensors.    





\section{Methodology}
This section presents the developed methods and their implementation. 

\subsection{Overview of Features and Components}
Our framework pAIrSEEption is a multimodal voice assistance system for environmental detection and navigation with a responsive graphical user interface (GUI) based on PySide6 (see Fig. \ref{framework_architecture}), which serves as the interface between humans and the machine. Fig.\ref{overview_framework} illustrates the features of the user interface. The GUI can be installed across various platforms. In this case, the interface was installed on the Jetson AGX Orin (32GB version), an edge device and on a standard laptop with an integrated Nvidia graphics card. Subsequently, a ZED 2i stereo camera can be connected to these devices. The mobile Husky robot is utilized as the mounting platform for the components as shown in Fig.\ref{husky_robot}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=2.5in]{images/overview_eng.png}
	\caption{Husky Robot as a mounting platform for the ZED 2i camera.}
	\label{overview_framework}
\end{figure}

\begin{figure}[!b]
\centering
\includegraphics[width=2.5in]{images/husky_zed.jpg}
\caption{Husky Robot as a mounting platform for the ZED 2i camera.}
\label{husky_robot}
\end{figure}

The GUI acts as a visualization and control center for the user. All data streams from the various components are bundled here. The interface displays the following visual information: 

\begin{itemize}
  \item{A live, annotated video stream of the stereo camera from the perception data pipeline.}
  \item{System performance statistics, such as frames per second (FPS) and the percentage utilization of the central processing unit (CPU), graphics processing unit (GPU), video random access memory (VRAM), and random-access memory (RAM).}
  \item{Status and system messages for active and inactive processes}
  \item{Transcribed user voice commands}
  \item{Scene descriptions and casual chat as text generated by the LLM, which are created using object data and images from the current scene}
  \item{Navigation control overview to select a recognized object to navigate to and/or to follow}
  \item{In addition, internal robot data such as motor temperatures can be queried via voice input and are displayed in the GUI}
\end{itemize}

All data and information displayed on the interface, as well as user (voice) input, can be saved locally in common file formats via the GUI, facilitating any necessary fine-tuning or training of the models (YOLO and MLLM) in the future.

\subsection{Setup and Applications}
The interface provides a selection of different versions and sizes of YOLO and YOLOE (open prompt) models for object detection and segmentation as well as MLLMs for scene interpretation, which can be expanded as needed.  In addition, specific YOLO settings can be configured, such as showing and hiding bounding boxes, classification, and confidence. With an additional confidence slide bar, objects that are not well recognized can be filtered, which are below the set threshold. The dataflow can be divided into five stages (Fig.\ref{framework_architecture}):
\begin{enumerate}
	\item Voice (Speech-To-Text, STT) or text input via the graphical user interface (scene description, navigation goal, robot status or casual chat). 
	\item Object detection with distance. If casual conversation with the MLLM is initiated then this step is skipped. 
	\item Preprocessing via rule-based intent analyzer depending on the input. This step could potentially be replaced with a small MLLM for orchestrating the tasks, but a rule-based approach offers lower latency and is more ressource efficient. In dynamic and critical situations, e.g., in road traffic, fast processing is a top priority.
	\item Depending on the intention, the response is output via MLLM or rule-based text (preconfigured response) if the situation requires it. In any case, the implemented text-to-speech system (TTS) reads out the answer. Complex scene descriptions or casual conversations are forwarded to MLLM, while the robot's status and navigation goals (following an object or simply driving to an object) are rule-based processes.    
	\item When navigation or system status is requested, the system writes or subscribes to ROS2 topics to control the robot or query its status.
\end{enumerate}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{images/framework_architecture.pdf}
	\caption{Overview of the system architecture.}
	\label{framework_architecture}
\end{figure*}

Efficient YOLO-based object detection serves three purposes in the system. First, YOLO provides real-time 2D detections that are fused with the ZED 2i depth measurements to estimate spatial object data (e.g., distance and 3D localization, see Fig. \ref{framework_architecture}), which are then formatted (Fig. \ref{transformation_template}) and passed to the MLLM as part of the grounded input prompt. Second, although modern MLLMs/VLMs can also produce 2D bounding boxes, YOLO is used as a lightweight preliminary detection layer to reduce MLLM workload and reduce inference latency.

[The ZED SDK provides high-resolution stereo images and a depth map, which shows the distance of each pixel in the image. It also offers position tracking and a velocity estimation of the objects via the Python API, pyzed. This API can be used to configure camera parameters, such as resolution, FPS, depth mode, and depth measurement units. The resolution is set to 1280 x 720 pixels by default, with 60 FPS and performance mode for depth detection enabled. The unit of measurement is set to meters. The origin of the stereo camera is set to the coordinates ($x = 0$, $y = 0$, $z = 0$). For compatibility reasons, the camera's 3D coordinate system has been set to match the one used in Robot Operating System 2 (ROS2), a middleware framework for robots, since the mobile robot is based on it.]

Third, it is used to navigate and follow a recognized object. To this end, the distance from each detected object to the camera and the pairwise inter-object distances within a frame are computed using the 3D Euclidean distance. The selection of the target object is initiated through its unique identification (ID), which is facilitated by either vocal input or interface selection. 


In pAIrSEEption, the objects \( \mathcal{O} = \{o_1, \ldots, o_n\} \) in the scene are schematically defined as follows (for example, a recognized person standing still):

%\[
%o_1 = \Bigl(\text{person}, \ 
%\left\{
%\begin{aligned}
%	& \text{ID} = 1, \\
%	& \text{confidence value} = 0.95, \\
%	& \text{3D position} = (x, y, z), \\
%	& \text{3D speed} = (v_x = 0, v_y = 0, v_z = 0), \\
%	& \text{speed classification} = \text{static}
%\end{aligned}
%\right\}
%\Bigr)
%\]


\begin{equation}
	\begin{aligned}
		o_1 = \Bigl(\text{person},\ 
		\left\{
		\begin{aligned}
			& \text{ID} = 1, \\
			& \text{confidence value} = 0.95, \\
			& \text{3D position} = (x, y, z), \\
			& \text{3D speed} = (v_x = 0, v_y = 0, v_z = 0), \\
			& \text{speed classification} = \text{static}
		\end{aligned}
		\right\}
		\Bigr)
	\end{aligned}
	\label{eq:o1}
\end{equation}


Where the object $o_1$ receives the attributes ID, confidence value, 3D positions, 3D velocity, and the classification \textit{static} if the velocity is zero. Thus, \textit{static} is an attribute derived from velocity, and the attribute set \textit{A} (see (\ref{eq:o_i})) is expanded with this additional information. The object data is expressed in the depth camera reference frame, i.e., the estimated 3D position ($x$, $y$, $z$) and the velocity ($v_x$, $v_y$, $v_z$) are relative to the camera.
The GUI offers various selections of MLLMs. These can be divided into two categories: 


The first category is local and offline models from Ollama such as Gemma 3 or Qwen 2.5 VL, hosted on consumer PCs 
in the laboratory or on an edge device like the Jetson AGX Orin attached to the mobile Husky Robot.
This solution offers the advantages of control, versioning, and data protection, but requires suitable hardware. Quantized and specialized or trainable models can be used for weaker hardware and a stable internet connection is not necessary. The second category is online, cloud hosted models via the OpenRouter API. With a single API key, it can connect to many different models from OpenAI, Google, Meta, Anthropic, etc. Most of these are subject to a fee per query and have a request limit. 
Nevertheless, inference takes place on the servers of the respective providers, meaning that significantly less powerful hardware (e.g., GPU) is required. In this case, factors such as server availability and data security are beyond the user's control. In potentially unstructured areas, connection problems may occur. For these reasons, both variants were implemented, which can be selected as needed and depending on internet availability and costs (resource costs and monetary costs).

The experiments (in SECTION X) entail the implementation of accessible MLLMs and VLMs on a local network server and the incorporation of the Jetson AGX Orin (32 GB version) into the mobile robot. The Jetson AGX Orin serves as the onboard PC, facilitating the comparison of onboard LLMs with fixed LLM models operating on more advanced hardware. Fig. \ref{client_server} depicts the implemented data pipeline, where the Jetson acts  as the client. The Jetson is used here as a representative PC on which the GUI application runs. However, we also show that even consumer notebooks can be used as a client.
Due to the computing capacity of the Jetson Orin (32 GB or 64 GB), it is also possible to use it as a server for smaller MLLMs.
\begin{figure}[!b]
	\centering
	\includegraphics[width=2.5in]{images/jetson_client_cropped.pdf}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{client_server}
\end{figure}

If a request for a description of the scene is initiated, the Jetson or laptop transmits an image of the current scene to the Ollama server—or, depending on the selection, also to OpenRouter—with the previously merged object data as a prompt to the server for interpretation and evaluation.
The following text-prompt instructs the MLLM on processing data. The goal is to enable safe navigation for a visually impaired person: 


%\begin{figure}[!htpb]
%	\centering
%	\includegraphics[width=2.5in]{images/grey_textbox.pdf}
%	\caption{Input prompt for the LLM to process with object data. Translated from German to English.}
%	\label{input_prompt}
%\end{figure}

%\begin{figure}[!htpb]
%	\centering
%	% define grey (5% black, 95% white)
%	\fcolorbox{black}{black!5}{%
%		\begin{minipage}{0.92\columnwidth} 
%			\setlength{\parskip}{0.5em} % paragraph distance
%			
%			``Describe the image briefly and precisely using the object data provided.
%			
%			\{object\_description\}
%			
%			Please describe what can be seen in the image,\\ 
%			where important objects are located,\\ 
%			and provide information about possible obstacles.''
%		\end{minipage}%
%	}
%	\caption{Input prompt for the LLM to process with object data (translated from German to English).}
%	\label{fig:llm_prompt}
%\end{figure}

%without fig. caption
%\vspace{1em} 
\noindent
\begin{center} 
	\fcolorbox{black}{black!5}{%
		\begin{minipage}{0.92\columnwidth}
			\setlength{\parskip}{0.5em}
			\ttfamily
			
			Describe the image briefly and precisely using the object data provided.
			
			\{object\_description\}
			
			Describe what can be seen in the image,
			where important objects are located,
			and provide information about possible obstacles.
		\end{minipage}%
	}
\end{center}
%\vspace{1em} 

The object data, defined as \textit{object\_description} is provided in the prompt and has been translated into concise bullet points during the preprocessing stage. Initially, a template for the raw data was employed. Fig. \ref{transformation_template} presents a schematic representation of the result of this transformation using the template. By combining the image data with the readable object data using the MLLM, an understandable 3D scene is described, which is accessible to humans.
The result of the scene interpretation is displayed as text in the output window of the GUI and can optionally be played back via an audio file generated by activating the text-to-speech function.

\begin{figure}[!b]
	\centering
	\includegraphics[width=\columnwidth]{images/transformation_template.png}
	\caption{Data preprocessing: Transformation of object data into a readable description for prompt input}
	\label{transformation_template}
\end{figure}


\begin{figure}[htpb]
	\centering
	\includegraphics[width=8.7cm]{images/yolo_timeline_yolo11l_tensorRT.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{yolo_timeline_tensorrt11}
\end{figure}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/timing_breakdown.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{timing_breakdown}
\end{figure}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/yolo_performance_1_of_2.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{yolo_performance_1_2}
\end{figure}

\section{Experiments and Results}



\section{Text}

\section{Conclusion}
The conclusion goes here.


\section*{Acknowledgments}
This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



{\appendix[Proof of the Zonklar Equations]
Use $\backslash${\tt{appendix}} if you have a single appendix:
Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
 starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}



\section{References Section}
You can use a bibliography generated by BibTeX as a .bbl file.
BibTeX documentation can be easily obtained at:
http://mirror.ctan.org/biblio/bibtex/contrib/doc/
The IEEEtran BibTeX style support page is:
http://www.michaelshell.org/tex/ieeetran/bibtex/

% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
\section{Simple References}
You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
(used to reserve space for the reference number labels box).

\begin{thebibliography}{1}
	\bibliographystyle{IEEEtran}
\bibitem{obrenovic2025generative}Obrenovic, B., Gu, X., Wang, G., Godinic, D. \& Jakhongirov, I. Generative AI and human–robot interaction: implications and future agenda for business, society and ethics. {\em AI \& Society}. \textbf{40}, 677-690 (2025)

	\bibitem{singh2024pushing}
	Singh, C. and Jayadas, A. Pushing Shopping Cart: An Intentional Exercise for Older Adults. {\em Activities, Adaptation \& Aging}. \textbf{48}, 670-689 (2024)
	
	\bibitem{gona2024intelligent}Gona, S. \& Harish, C. Intelligent mobility planning for a cost-effective object follower mobile robotic system with obstacle avoidance using robot vision and deep learning. {\em Evolutionary Intelligence}. \textbf{17}, 1279-1293 (2024)
	
	\bibitem{conceptgraphs}
	Q. Gu et al., "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5021-5028, doi: 10.1109/ICRA57147.2024.10610243.
	
	\bibitem{3dgraphllm}
	Tatiana Zemskova and Dmitry Yudin, "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding," 2025, https://doi.org/10.48550/arXiv.2412.18450
	
	\bibitem{graph2nav}
	T. Shan, A. Rajvanshi, N. Mithun and H. -P. Chiu, "Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation," 2025 IEEE International Conference on Robotics and Automation (ICRA), Atlanta, GA, USA, 2025, pp. 1646-1652, doi: 10.1109/ICRA55743.2025.11128782. 
	
	\bibitem{hovsg}
	Robotics: Science and Systems 2024, Delft, Netherlands, July 15-July 19, 2024, 
	https://doi.org/10.15607/RSS.2024.XX.077
	
	\bibitem{openin}
	Y. Tang et al., "OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic Environments," in IEEE Robotics and Automation Letters, vol. 10, no. 9, pp. 9256-9263, Sept. 2025, doi: 10.1109/LRA.2025.3592071.
	
	\bibitem{sgnav}
	H. Yin et al., "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation,"
	https://doi.org/10.48550/arXiv.2410.08189
	
	\bibitem{interaction}
	Q. Li, X. Zhang, C. Chen, H. Zhao and J. Niu, "Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution," 2025 IEEE International Conference on Robotics and Automation (ICRA), Atlanta, GA, USA, 2025, pp. 11933-11939, doi: 10.1109/ICRA55743.2025.11128194.
	
	\bibitem{visiongpt}
	H. Wang et al., "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation,"
	https://doi.org/10.48550/arXiv.2403.12415
	
	\bibitem{lgx}
	V. S. Dorbala, J. F. Mullen and D. Manocha, "Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation," in IEEE Robotics and Automation Letters, vol. 9, no. 5, pp. 4083-4090, May 2024, doi: 10.1109/LRA.2023.3346800.
	
	\bibitem{vlmaps}
	C. Huang, O. Mees, A. Zeng and W. Burgard, "Visual Language Maps for Robot Navigation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 10608-10615, doi: 10.1109/ICRA48891.2023.10160969.
	
\end{thebibliography}


\newpage

\section{Biography Section}
If you have an EPS/PDF photo (graphicx package needed), extra braces are
needed around the contents of the optional argument to biography to prevent
the LaTeX parser from getting confused when it sees the complicated
$\backslash${\tt{includegraphics}} command within an optional argument. (You can create
your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
simpler here.)

\vspace{11pt}

\bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
	Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
	Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

\vspace{11pt}

\bf{If you will not include a photo:}\vspace{-33pt}
\begin{IEEEbiographynophoto}{John Doe}
	Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
\end{IEEEbiographynophoto}




\vfill

\end{document}


