\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

%added xcolor for box color
\usepackage{xcolor}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{LLM-based scene graph for Outdoor Human-Robot \\ Teaming in Society 6.0 – Design, applications, and \\ evaluation}

\author{Trung Kien La and Eric Guiffo Kaigom
        % <-this % stops a space
\thanks{Trung Kien La was with the Frankfurt Industrial Robotics and Digital Twin Laboratory (FriiDA) at the Frankfurt University of Applied Sciences, Hungener Str. 6 Building C, 60389 Frankfurt am Main, Germany (Orcid: 0009-0007-9239-3268).}% <-this % stops a space
\thanks{Eric Guiffo Kaigom is with the Frankfurt Industrial Robotics and Digital Twin Laboratory (FriiDA) at the Frankfurt University of Applied Sciences, Hungener Str. 6 Building C, 60389 Frankfurt am Main, Germany (e-mail: kaigom@fra-uas.de).}}

% The paper headers
\markboth{Replace this line with your manuscript id number}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
—[Society 5.0/6.0 here?]…This article further describes the design, application and implementation of a multimodal assistance system to support [visually impaired] persons. To this end, a mobile robot is equipped with a depth camera, object detection and large language models (LLMs). The goal is to enable the robot to understand its environment, i.e., to perceive, understand, and respond to it. Additionally, the robot should describe its perceived environment in natural language. A graphical user interface has been developed to bundle all asynchronous processes and act as a central audiovisual control unit. Visual perception is achieved through data fusion of the depth camera and object recognition models. The resulting 3D object data is then used to implement robot navigation. Human-machine interaction takes place via a voice interface that uses speech recognition and a text-to-speech system for speech output. To enrich the scene description, local multimodal LLMs are used. For this purpose, a client-server architecture was established. [mention of results and conducted experiments?] …[reference/benefits to Society 6.0?]…. 
\end{abstract}

\begin{IEEEkeywords}
Article submission, IEEE, IEEEtran, journal, \LaTeX, paper, template, typesetting.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{T}{he} society of the future will integrate emerging technologies to promote human well-being and enhance quality of life in sustainable and equitable ways. A key objective of this transformation is the establishment of accountable corporate leadership that responsibly affects stakeholders and their environments. Equally important are collective intelligence and the self-determination of individuals in their increasingly mobile professional and social activities. However, the effectiveness of distributed, interconnected, and inclusive autonomy requires perceptual and interactive capacities that extend beyond human limitations, particularly those related to endurance, precision, and feedback in motor skills, vision, and cognition.
AI-powered technologies are thus designed to integrate data processing, intelligent interfaces, and adaptive actuators to inform, augment, and extend human abilities. These systems enhance situational awareness while reducing cognitive load, strengthen personalized responses to societal challenges, and promote flexible, natural interaction with minimal effort. The use of environmentally sustainable and everyone-accessible technologies—as personal assistants in home care, adaptive interfaces in rehabilitation, and immersive tools in education and training—can mitigate cultural, physical and cognitive barriers while fostering new individual capacities and professional opportunities.
Service robots constitute a prominent class of such technologies. They increasingly demonstrate their capacity to simplify and streamline daily human activities. Robots can transport heavy or critical payloads at customizable speeds with high precision. These capabilities can be achieved in a climate-friendly manner when powered by renewable energy sources such as solar and wind systems. Despite this societal and environmental potential, robot adoption remains constrained by factors beyond cost and safety. First, current systems lack functionalities that enable a transition from pre-scripted task execution to broader perception, interpretation, and real-time knowledge sharing with humans. Second, non-specialists often face difficulties in translating high-level task intentions into robotic actions. Third, existing designs seldom support synergy and role switching between humans and robots during joint task execution. Fourth, robots typically fail to communicate their internal states and feedback in accessible and intelligible ways.
To address these limitations, this work presents an AI-driven infrastructure for human–robot teaming across a wide spectrum of societal applications. These include indoor assistance for the elderly—in which case robots act as followers or commissioned agents for payload transport—and outdoor guidance for individuals with visual impairments. Large Language Models (LLMs) are leveraged to establish a bidirectional, natural-language communication between humans and robots. By exploiting linguistic subtleties, the system dynamically acquires contextual understanding and supports high-level task planning in interchangeable languages, thereby fostering and accelerating inclusion, participation, and collective intelligence.
The robot operates on a scene graph that delivers a multimodal, structured representation of its environment and internal behavior, encompassing potential obstacles and energy consumption. This comprehensive model enables the robot to explain its decisions and offer quantitative insights into their underlying causes upon request. Such feedback facilitates adaptive task refinement and energy optimization. Furthermore, the robot is equipped with master and follower capabilities that support dynamic role switching. Humans retain strategic decision-making authority, guided by information provided by the robot. The resulting extended machine intelligence demonstrates utility across diverse applications, from assistive domestic environments to outdoor navigation and cooperative task execution.


\begin{table*}[!hb]
	\centering
	\caption{Technical Overview of Models and Technologies for 3D Scene Graph Generation, Perception, Navigation and Interfacing}
	\label{tab:scenegraph_technologies}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{System/Framework} & \textbf{\begin{tabular}[c]{@{}l@{}}Detection \& Segmentation \\ (Nodes)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Semantic Features \& \\ Embeddings\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Relation Inference \& \\ Planning (Edges)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Alternatives \& \\ Additional Models \& \\ Misc. \end{tabular}}  \\ \hline
			\textbf{ConceptGraphs}\cite{conceptgraphs} & SAM (Segment Anything) & CLIP, Grounding DINO & GPT-4, LLaVA & RAM \\ \hline
			\textbf{3DGraphLLM}\cite{3dgraphllm} & Mask3D, OneFormer3D & DINOv2 (2D), Uni3D (3D) & VL-SAT, CLIP & LLaMA3-8B, Vicuna-1.5 \\ \hline
			\textbf{Graph2Nav}\cite{graph2nav} & PSGFormer, Mask2Former & Panoptic Labels & 2D-PSG Inference, GPT-3.5 & LIO-SAM, Whisper \\ \hline
			\textbf{HOV-SG}\cite{hovsg} & SAM & CLIP & Hierarchical Heuristics & GPT-3.5, GPT-4 \\ \hline
			\textbf{OpenIN}\cite{openin} & CropFormer & SBERT, CLIP & GPT-4o & Tokenize Anything \\ \hline
			\textbf{SG-Nav}\cite{sgnav} & SAM & VLMs, LLMs & LLaMA-7B, GPT-4, LLaVA & Fast Marching Method \\ \hline
			\textbf{Interaction Updates}\cite{interaction} & Existing 3D SG (e.g., Hydra) & Visual Transformer (ViT) & GPT-4, BLIP-2 & DBSCAN (Denoising) \\ \hline
			\textbf{VisionGPT}\cite{visiongpt} & YOLO-World & Proportional 2D Coordinates & GPT-3.5 Turbo, GPT-4 & H-Pattern Splitter \\ \hline
			\textbf{LGX}\cite{lgx} & GLIP, YOLO & BLIP (Captions) & GPT-3 & OWL-ViT, Detectron2 \\ \hline
			\textbf{VLMaps}\cite{vlmaps} & LSeg & CLIP & Codex (Code-writing LLM) & RTAB-Map \\ \hline
		\end{tabular}%
	}
\end{table*}

\section{Related Work}
\begin{itemize}
\item{First short definition of scene graph}
\item{Comparison with other scene graph generation tools and frameworks such as [Concept Graph, OpenIN, HOVSG, Graph2Nav, 3DGraphLLM, SG-Nav, Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution, VLMaps ]  (need mapping beforehand and/or need powerful and additional hardware for models such as Grounding DINO, CLIP, GLIP, SAM, BLIP-2 and even additional drones for mapping, which does not suit our case [real-time, dynamic and unseen environment, reactive, portable/mobile, cost-effective or free/local models, audio interaction, suitable outdoors]). Nearly all the frameworks are used indoors.}
\item{Most similar:  }
\item{Can an Embodied Agent Find Your –Cat-shaped Mug? LLM-Based Zero-Shot Object Navigation (uses depth information, but needs mapping)}
\item{VISIONGPT (YOLO World, ChatGPT, voice input and feedback)}
\item {Cobots \& Service Robots \& Society 5.0/6.0}
\end{itemize}

A scene graph (SG) serves to represent a scene with objects,
their attributes, and relationships between other objects in a structured way.
Scene graph generation (SGG) can be based on images, text, and videos.
Such a hierarchical graph data structure consists of nodes (O) and edges (E). Nodes (O) are objects such as persons, animals, places or things (bicycle, car, etc.), but also parts of other objects (e.g., limbs such as a person's arms). Each object node is usually equipped with attributes that describe the state of the object (e.g., colors, size, or pose). The edges (E) represent the relationships between the object pairs. These connections between objects describe actions (man holds cup) or positions (man stands in front of a bench). Typically, such a relationship is expressed as a
$\langle \text{subject} \;-\; \text{predicate} \;-\; \text{object} \rangle$ triple.
An SG can be defined as a tuple:

\begin{equation}
	\label{eq:SG}
	SG = (\mathcal{O}, R, E)
\end{equation}

\noindent Where: 

\begin{itemize}
	\item $\mathcal{O} = \{o_1, \ldots, o_n\}$ is the set of objects recognized in the images, 
	where $n$ is the number of objects. Each object $o_i$ can also be described as a tuple: 
	\begin{equation}
		\label{eq:o_i}
		o_i = (c_i, A_i)
	\end{equation}
	where $c_i$ denotes the category (e.g., person, soccer stadium, animal) and 
	$A_i$ denotes the attributes (e.g., shape, color, pose) of the object.
	
	\item $R$ stands for the set of relationships between the nodes (objects).
	\item $E \subseteq \mathcal{O} \times R \times \mathcal{O}$ represents
	the edges between the object instance nodes 
	and the relationship nodes. The edge set $E$ consists of exactly those triples that are considered 
	relationships between the object pairs.
\end{itemize}
	
\noindent Using the following formulation, scene graphs can be expressed from an image $I$ using SGG methods:
	\begin{equation}
		\label{eq:SGG}
		SG(\mathcal{O}, R, E) = SGG(I)
	\end{equation}
	
\noindent For SGG, diverse models and technologies are employed across robotics, computer vision, autonomous navigation, and human-machine interaction. These are summarized in Table \ref{tab:scenegraph_technologies}. 
Due to the steadily increasing availability and improvements of LLMs, over the years there has been a shift from simple object recognition (with limited classes) and bounding boxes to open-vocabulary models (e.g., SAM, CLIP) and pixel-accurate masks, as well as the use of VLMs that recognize the semantic states and relationships of objects.
Frameworks such as ConceptGraphs and 3DGraphLLM use this semantic information to make graphs interpretable for LLMs and also enrich the objects with geometric metadata obtained from sensors such as LiDAR or depth cameras. Approaches such as VLMaps and Graph2Nav focus on directly translating these spatial relations into executable navigation commands.




\section{Methodology}
This section presents the developed methods and their implementation. 

\subsection{Overview of Features and Components}
Our framework pAIrSEEption is a multimodal voice assistance system for environmental detection and navigation with a responsive graphical user interface (GUI) based on PySide6 (see Fig. \ref{framework_architecture}), which serves as the interface between humans and the machine. Fig.\ref{overview_framework} illustrates the features of the user interface. The GUI can be installed across various platforms. In this case, the interface was installed on the Jetson AGX Orin (32GB version), an edge device and on a standard laptop with an integrated Nvidia graphics card. Subsequently, a ZED 2i stereo camera can be connected to these devices. The mobile Husky robot is utilized as the mounting platform for the components as shown in Fig.\ref{husky_robot}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=2.5in]{images/overview_eng.png}
	\caption{Husky Robot as a mounting platform for the ZED 2i camera.}
	\label{overview_framework}
\end{figure}

\begin{figure}[!b]
\centering
\includegraphics[width=2.5in]{images/husky_zed.jpg}
\caption{Husky Robot as a mounting platform for the ZED 2i camera.}
\label{husky_robot}
\end{figure}

The GUI acts as a visualization and control center for the user. All data streams from the various components are bundled here. The interface displays the following visual information: 

\begin{itemize}
  \item{A live, annotated video stream of the stereo camera from the perception data pipeline.}
  \item{System performance statistics, such as frames per second (FPS) and the percentage utilization of the central processing unit (CPU), graphics processing unit (GPU), video random access memory (VRAM), and random-access memory (RAM).}
  \item{Status and system messages for active and inactive processes}
  \item{Transcribed user voice commands}
  \item{Scene descriptions and casual chat as text generated by the LLM, which are created using object data and images from the current scene}
  \item{Navigation control overview to select a recognized object to navigate to and/or to follow}
  \item{In addition, internal robot data such as motor temperatures can be queried via voice input and are displayed in the GUI}
\end{itemize}

All data and information displayed on the interface, as well as user (voice) input, can be saved locally in common file formats via the GUI, facilitating any necessary fine-tuning or training of the models (YOLO and LLM) in the future.

\subsection{Setup and Applications}
The interface provides a selection of different versions and sizes of YOLO(e) models for object detection and segmentation as well as LLMs for scene interpretation, which can be expanded as needed.  In addition, specific YOLO settings can be configured, such as showing and hiding bounding boxes, classification, and confidence. With an additional confidence slide bar, objects that are not well recognized can be filtered, which are below the set threshold. The YOLO object detection is leveraged for two functions. First, it is used for data fusion with the ZED 2i depth camera to provide object data (see Fig. \ref{framework_architecture}), which is then fed to the LLM as a prompt. [The ZED SDK provides high-resolution stereo images and a depth map, which shows the distance of each pixel in the image. It also offers position tracking and a velocity estimation of the objects via the Python API, pyzed. This API can be used to configure camera parameters, such as resolution, FPS, depth mode, and depth measurement units. The resolution is set to 1280 x 720 pixels by default, with 60 FPS and performance mode for depth detection enabled. The unit of measurement is set to meters. The origin of the stereo camera is set to the coordinates (x=0, y=0, z=0). For compatibility reasons, the camera's 3D coordinate system has been set to match the one used in Robot Operating System 2 (ROS2), a middleware framework for robots, since the mobile robot is based on it.]
Second, it is used to navigate and follow a recognized object. To this end, the distance from each detected object to the camera and the pairwise inter-object distances within a frame are computed using the 3D Euclidean distance. The selection of the target object is initiated through its unique identification (ID), which is facilitated by either vocal input or interface selection.

In pAIrSEEption, the objects \( \mathcal{O} = \{o_1, \ldots, o_n\} \) in the scene are schematically defined as follows (for example, a recognized person standing still):

%\[
%o_1 = \Bigl(\text{person}, \ 
%\left\{
%\begin{aligned}
%	& \text{ID} = 1, \\
%	& \text{confidence value} = 0.95, \\
%	& \text{3D position} = (x, y, z), \\
%	& \text{3D speed} = (v_x = 0, v_y = 0, v_z = 0), \\
%	& \text{speed classification} = \text{static}
%\end{aligned}
%\right\}
%\Bigr)
%\]


\begin{equation}
	\begin{aligned}
		o_1 = \Bigl(\text{person},\ 
		\left\{
		\begin{aligned}
			& \text{ID} = 1, \\
			& \text{confidence value} = 0.95, \\
			& \text{3D position} = (x, y, z), \\
			& \text{3D speed} = (v_x = 0, v_y = 0, v_z = 0), \\
			& \text{speed classification} = \text{static}
		\end{aligned}
		\right\}
		\Bigr)
	\end{aligned}
	\label{eq:o1}
\end{equation}








Where the object $o_1$ receives the attributes ID, confidence value, 3D positions, 3D velocity, and the classification \textit{static} if the velocity is zero. Thus, \textit{static} is an attribute derived from velocity, and the attribute set \textit{A} (see (\ref{eq:o_i})) is expanded with this additional information. The selection of LLMs can be divided into two categories: 

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{images/framework_architecture.pdf}
	\caption{Overview of the system architecture.}
	\label{framework_architecture}
\end{figure*}


The first category is local and offline models from Ollama or other sources such as Gemma 3 or Qwen 2.5 VL, hosted on consumer PCs 
in the laboratory or on an edge device like the Jetson AGX Orin attached to the mobile Husky Robot.
This solution offers the advantages of control, versioning, and data protection, but requires suitable hardware. Quantized and specialized or trainable models can be used for weaker hardware and a stable internet connection is not necessary. The second category is online, hosted models via the OpenRouter API. With a single API key, it can connect to many different models from OpenAI, Google, Meta, Anthropic, etc. Most of these are subject to a fee per query and have a request limit. 
Nevertheless, inference takes place on the servers of the respective providers, meaning that significantly less powerful hardware (e.g., GPU) is required. In this case, factors such as server availability and data security are beyond the user's control. In potentially unstructured areas, connection problems may occur. For these reasons, both variants were implemented, which can be selected as needed and depending on Internet availability and costs (resource costs and monetary costs).

The experiments (in SECTION X) entail the implementation of accessible LLMs and VLMs on a local network server and the incorporation of the Jetson AGX Orin (32 GB version) into the mobile robot. The Jetson AGX Orin serves as the onboard PC, facilitating the comparison of onboard LLMs with fixed LLM models operating on more advanced hardware. Fig. \ref{client_server} depicts the implemented data pipeline, where the Jetson acts  as the client. The Jetson is used here as a representative PC on which the GUI application runs. However, we also show that even consumer notebooks can be used as a client.
Due to the computing capacity of the Jetson, it is also possible to use it as a server for smaller LLMs.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=2.5in]{images/jetson_client_cropped.pdf}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{client_server}
\end{figure}

If a request for a description of the scene is initiated, the Jetson or laptop transmits an image of the current scene to the Ollama server—or, depending on the selection, also to OpenRouter—with the previously merged object data as a prompt to the server for interpretation and evaluation.
The following text-prompt instructs the LLM on processing data. The goal is to enable safe navigation for a visually impaired person: 


%\begin{figure}[!htpb]
%	\centering
%	\includegraphics[width=2.5in]{images/grey_textbox.pdf}
%	\caption{Input prompt for the LLM to process with object data. Translated from German to English.}
%	\label{input_prompt}
%\end{figure}

%\begin{figure}[!htpb]
%	\centering
%	% define grey (5% black, 95% white)
%	\fcolorbox{black}{black!5}{%
%		\begin{minipage}{0.92\columnwidth} 
%			\setlength{\parskip}{0.5em} % paragraph distance
%			
%			``Describe the image briefly and precisely using the object data provided.
%			
%			\{object\_description\}
%			
%			Please describe what can be seen in the image,\\ 
%			where important objects are located,\\ 
%			and provide information about possible obstacles.''
%		\end{minipage}%
%	}
%	\caption{Input prompt for the LLM to process with object data (translated from German to English).}
%	\label{fig:llm_prompt}
%\end{figure}

%without fig. caption
%\vspace{1em} 
\noindent
\begin{center} 
	\fcolorbox{black}{black!5}{%
		\begin{minipage}{0.92\columnwidth}
			\setlength{\parskip}{0.5em}
			\ttfamily
			
			Describe the image briefly and precisely using the object data provided.
			
			\{object\_description\}
			
			Please describe what can be seen in the image,
			where important objects are located,
			and provide information about possible obstacles.
		\end{minipage}%
	}
\end{center}
%\vspace{1em} 

The object data, defined as \textit{object\_description} is provided in the prompt and has been translated into concise bullet points during the preprocessing stage. Initially, a template for the raw data was employed. Fig. \ref{fig:transformation_template} presents a schematic representation of the result of this transformation using the template. The integration of image data with readable object data through the utilization of an LLM facilitates the creation of a comprehensive and cohesive representation of the subject matter. 
The description provides a detailed interpretation of a 3D scene that is accessible to humans. The result of the scene interpretation is displayed as text in the output window of the GUI and can optionally be played back via an audio file generated by activating the text-to-speech function.

\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth]{images/transformation_template.png}
	\caption{Data preprocessing: Transformation of object data into a readable description for prompt input}
	\label{fig:transformation_template}
\end{figure}

\subsection{Navigation and motion control}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/yolo_timeline_yolo11l_tensorRT.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{yolo_timeline_tensorrt11}
\end{figure}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/timing_breakdown.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{timing_breakdown}
\end{figure}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/yolo_performance_1_of_2.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{yolo_performance_1_2}
\end{figure}
\section{Other Resources}
See \cite{ref1,ref2,ref3,ref4,ref5} for resources on formatting math into text and additional help in working with \LaTeX .

\section{Text}

\section{Conclusion}
The conclusion goes here.


\section*{Acknowledgments}
This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



{\appendix[Proof of the Zonklar Equations]
Use $\backslash${\tt{appendix}} if you have a single appendix:
Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
 starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}



\section{References Section}
You can use a bibliography generated by BibTeX as a .bbl file.
 BibTeX documentation can be easily obtained at:
 http://mirror.ctan.org/biblio/bibtex/contrib/doc/
 The IEEEtran BibTeX style support page is:
 http://www.michaelshell.org/tex/ieeetran/bibtex/
 
 % argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
\section{Simple References}
You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
 (used to reserve space for the reference number labels box).

\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}

\bibitem{conceptgraphs}
Q. Gu et al., "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5021-5028, doi: 10.1109/ICRA57147.2024.10610243.

\bibitem{3dgraphllm}
Tatiana Zemskova and Dmitry Yudin, "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding," 2025, https://doi.org/10.48550/arXiv.2412.18450

\bibitem{graph2nav}
T. Shan, A. Rajvanshi, N. Mithun and H. -P. Chiu, "Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation," 2025 IEEE International Conference on Robotics and Automation (ICRA), Atlanta, GA, USA, 2025, pp. 1646-1652, doi: 10.1109/ICRA55743.2025.11128782. 

\bibitem{hovsg}
Robotics: Science and Systems 2024, Delft, Netherlands, July 15-July 19, 2024, 
https://doi.org/10.15607/RSS.2024.XX.077

\bibitem{openin}
Y. Tang et al., "OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic Environments," in IEEE Robotics and Automation Letters, vol. 10, no. 9, pp. 9256-9263, Sept. 2025, doi: 10.1109/LRA.2025.3592071.

\bibitem{sgnav}
H. Yin et al., "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation,"
https://doi.org/10.48550/arXiv.2410.08189

\bibitem{interaction}
Q. Li, X. Zhang, C. Chen, H. Zhao and J. Niu, "Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution," 2025 IEEE International Conference on Robotics and Automation (ICRA), Atlanta, GA, USA, 2025, pp. 11933-11939, doi: 10.1109/ICRA55743.2025.11128194.

\bibitem{visiongpt}
H. Wang et al., "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation,"
https://doi.org/10.48550/arXiv.2403.12415

\bibitem{lgx}
V. S. Dorbala, J. F. Mullen and D. Manocha, "Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation," in IEEE Robotics and Automation Letters, vol. 9, no. 5, pp. 4083-4090, May 2024, doi: 10.1109/LRA.2023.3346800.

\bibitem{vlmaps}
C. Huang, O. Mees, A. Zeng and W. Burgard, "Visual Language Maps for Robot Navigation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 10608-10615, doi: 10.1109/ICRA48891.2023.10160969.

\end{thebibliography}


\newpage

\section{Biography Section}
If you have an EPS/PDF photo (graphicx package needed), extra braces are
 needed around the contents of the optional argument to biography to prevent
 the LaTeX parser from getting confused when it sees the complicated
 $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
 your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
 simpler here.)
 
\vspace{11pt}

\bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

\vspace{11pt}

\bf{If you will not include a photo:}\vspace{-33pt}
\begin{IEEEbiographynophoto}{John Doe}
Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
\end{IEEEbiographynophoto}




\vfill

\end{document}


