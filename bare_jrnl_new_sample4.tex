\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

%added xcolor for box color
\usepackage{xcolor}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{LLM-based scene graph for Outdoor Human-Robot \\ Teaming in Society 6.0 – Design, applications, and \\ evaluation}

\author{Trung Kien La and Eric Guiffo Kaigom
        % <-this % stops a space
\thanks{Trung Kien La was with the Frankfurt Industrial Robotics and Digital Twin Laboratory (FriiDA) at the Frankfurt University of Applied Sciences, Hungener Str. 6 Building C, 60389 Frankfurt am Main, Germany (Orcid: 0009-0007-9239-3268).}% <-this % stops a space
\thanks{Eric Guiffo Kaigom is with the Frankfurt Industrial Robotics and Digital Twin Laboratory (FriiDA) at the Frankfurt University of Applied Sciences, Hungener Str. 6 Building C, 60389 Frankfurt am Main, Germany (e-mail: kaigom@fra-uas.de).}}

% The paper headers
\markboth{Replace this line with your manuscript id number}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
—[Society 5.0/6.0 here?]…This article describes the design, application and implementation of a multimodal assistance system to support [visually impaired] persons. To this end, a mobile robot is equipped with a depth camera, object detection and large language models (LLMs). The goal is to enable the robot to understand its environment, i.e., to perceive, understand, and respond to it. Additionally, the robot should describe its perceived environment in natural language. A graphical user interface has been developed to bundle all asynchronous processes and act as a central audiovisual control unit. Visual perception is achieved through data fusion of the depth camera and object recognition models. The resulting 3D object data is then used to implement robot navigation. Human-machine interaction takes place via a voice interface that uses speech recognition and a text-to-speech system for speech output. To enrich the scene description, local multimodal LLMs are used. For this purpose, a client-server architecture was established. [mention of results and conducted experiments?] …[reference/benefits to Society 6.0?]…. 
\end{abstract}

\begin{IEEEkeywords}
Article submission, IEEE, IEEEtran, journal, \LaTeX, paper, template, typesetting.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{T}{he} society of the future will integrate emerging technologies to promote human well-being and enhance quality of life in sustainable and equitable ways. A key objective of this transformation is the establishment of accountable corporate leadership that responsibly affects stakeholders and their environments. Equally important are collective intelligence and the self-determination of individuals in their increasingly mobile professional and social activities. However, the effectiveness of distributed, interconnected, and inclusive autonomy requires perceptual and interactive capacities that extend beyond human limitations, particularly those related to endurance, precision, and feedback in motor skills, vision, and cognition.
AI-powered technologies are thus designed to integrate data processing, intelligent interfaces, and adaptive actuators to inform, augment, and extend human abilities. These systems enhance situational awareness while reducing cognitive load, strengthen personalized responses to societal challenges, and promote flexible, natural interaction with minimal effort. The use of environmentally sustainable and everyone-accessible technologies—as personal assistants in home care, adaptive interfaces in rehabilitation, and immersive tools in education and training—can mitigate cultural, physical and cognitive barriers while fostering new individual capacities and professional opportunities.
Service robots constitute a prominent class of such technologies. They increasingly demonstrate their capacity to simplify and streamline daily human activities. Robots can transport heavy or critical payloads at customizable speeds with high precision. These capabilities can be achieved in a climate-friendly manner when powered by renewable energy sources such as solar and wind systems. Despite this societal and environmental potential, robot adoption remains constrained by factors beyond cost and safety. First, current systems lack functionalities that enable a transition from pre-scripted task execution to broader perception, interpretation, and real-time knowledge sharing with humans. Second, non-specialists often face difficulties in translating high-level task intentions into robotic actions. Third, existing designs seldom support synergy and role switching between humans and robots during joint task execution. Fourth, robots typically fail to communicate their internal states and feedback in accessible and intelligible ways.
To address these limitations, this work presents an AI-driven infrastructure for human–robot teaming across a wide spectrum of societal applications. These include indoor assistance for the elderly—in which case robots act as followers or commissioned agents for payload transport—and outdoor guidance for individuals with visual impairments. Large Language Models (LLMs) are leveraged to establish a bidirectional, natural-language communication between humans and robots. By exploiting linguistic subtleties, the system dynamically acquires contextual understanding and supports high-level task planning in interchangeable languages, thereby fostering and accelerating inclusion, participation, and collective intelligence.
The robot operates on a scene graph that delivers a multimodal, structured representation of its environment and internal behavior, encompassing potential obstacles and energy consumption. This comprehensive model enables the robot to explain its decisions and offer quantitative insights into their underlying causes upon request. Such feedback facilitates adaptive task refinement and energy optimization. Furthermore, the robot is equipped with master and follower capabilities that support dynamic role switching. Humans retain strategic decision-making authority, guided by information provided by the robot. The resulting extended machine intelligence demonstrates utility across diverse applications, from assistive domestic environments to outdoor navigation and cooperative task execution.


\section{Related Work}
\begin{itemize}
\item{First short definition of scene graph}
\item{Comparison with other scene graph generation tools and frameworks such as [Concept Graph, OpenIN, HOVSG, Graph2Nav, 3DGraphLLM, SG-Nav, Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution, VLMaps ]  (need mapping beforehand and/or need powerful and additional hardware for models such as Grounding DINO, CLIP, GLIP, SAM, BLIP-2 and even additional drones for mapping, which does not suit our case [real-time, dynamic and unseen environment, reactive, portable/mobile, cost-effective or free/local models, audio interaction, suitable outdoors]). Nearly all the frameworks are used indoors.}
\item{Most similar:  }
\item{Can an Embodied Agent Find Your –Cat-shaped Mug? LLM-Based Zero-Shot Object Navigation (uses depth information, but needs mapping)}
\item{VISIONGPT (YOLO World, ChatGPT, voice input and feedback)}
\item {Cobots \& Service Robots \& Society 5.0/6.0}
\end{itemize}

A scene graph (SG) serves to represent a scene with objects,
their attributes, and relationships between other objects in a structured way.
Scene graph generation (SGG) can be based on images, text, and videos.
Such a hierarchical graph data structure consists of nodes (O) and edges (E). Nodes (O) are objects such as persons, animals, places or things (bicycle, car, etc.), but also parts of other objects (e.g., limbs such as a person's arms). Each object node is usually equipped with attributes that describe the state of the object (e.g., colors, size, or pose). The edges (E) represent the relationships between the object pairs. These connections between objects describe actions (man holds cup) or positions (man stands in front of a bench). Typically, such a relationship is expressed as a
$\langle \text{subject} \;-\; \text{predicate} \;-\; \text{object} \rangle$ triple.
An SG can be defined as a tuple:

\begin{equation}
	SG = (\mathcal{O}, R, E)
\end{equation}

\noindent Where: 

\begin{itemize}
	\item $\mathcal{O} = \{o_1, \ldots, o_n\}$ is the set of objects recognized in the images, 
	where $n$ is the number of objects. Each object $o_i$ can also be described as a tuple: 
	\begin{equation}
		o_i = (c_i, A_i)
	\end{equation}
	where $c_i$ denotes the category (e.g., person, soccer stadium, animal) and 
	$A_i$ denotes the attributes (e.g., shape, color, pose) of the object.
	
	\item $R$ stands for the set of relationships between the nodes (objects).
	\item $E \subseteq \mathcal{O} \times R \times \mathcal{O}$ represents
	the edges between the object instance nodes 
	and the relationship nodes. The edge set $E$ consists of exactly those triples that are considered 
	relationships between the object pairs.
\end{itemize}
	
\noindent Using the following formulation, scene graphs can be expressed from an image $I$ using SGG methods:
	\begin{equation}
		SG(\mathcal{O}, R, E) = SGG(I)
	\end{equation}
	
\noindent SGG employs diverse models and technologies across robotics, computer vision, autonomous navigation, and human-machine interaction. These are summarized in Table \ref{tab:scenegraph_technologies}.

	

\begin{table*}[t]
	\centering
	\caption{Technical Overview of Models and Technologies for 3D Scene Graph Generation, Perception, Navigation and Interfacing}
	\label{tab:scenegraph_technologies}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{System/Framework} & \textbf{\begin{tabular}[c]{@{}l@{}}Detection \& Segmentation \\ (Nodes)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Semantic Features \& \\ Embeddings\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Relation Inference \& \\ Planning (Edges)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Alternatives \& \\ Additional Models \& \\ Misc. \end{tabular}}  \\ \hline
			\textbf{ConceptGraphs} & SAM (Segment Anything) & CLIP, Grounding DINO & GPT-4, LLaVA & RAM \\ \hline
			\textbf{3DGraphLLM} & Mask3D, OneFormer3D & DINOv2 (2D), Uni3D (3D) & VL-SAT, CLIP & LLaMA3-8B, Vicuna-1.5 \\ \hline
			\textbf{Graph2Nav} & PSGFormer, Mask2Former & Panoptic Labels & 2D-PSG Inference, GPT-3.5 & LIO-SAM, Whisper \\ \hline
			\textbf{HOV-SG} & SAM & CLIP & Hierarchical Heuristics & GPT-3.5, GPT-4 \\ \hline
			\textbf{OpenIN} & CropFormer & SBERT, CLIP & GPT-4o & Tokenize Anything \\ \hline
			\textbf{SG-Nav} & SAM & VLMs, LLMs & LLaMA-7B, GPT-4, LLaVA & Fast Marching Method \\ \hline
			\textbf{Interaction Updates} & Existing 3D SG (e.g., Hydra) & Visual Transformer (ViT) & GPT-4, BLIP-2 & DBSCAN (Denoising) \\ \hline
			\textbf{VisionGPT} & YOLO-World & Proportional 2D Coordinates & GPT-3.5 Turbo, GPT-4 & H-Pattern Splitter \\ \hline
			\textbf{LGX} & GLIP, YOLO & BLIP (Captions) & GPT-3 & OWL-ViT, Detectron2 \\ \hline
			\textbf{VLMaps} & LSeg & CLIP & Codex (Code-writing LLM) & RTAB-Map \\ \hline
		\end{tabular}%
	}
\end{table*}




\section{Methodology}
This section presents the developed methods and their implementation. 

\subsection{Overview of Features and Components}
Our framework pAIrSEEption is a multimodal voice assistance system for environmental detection and navigation with a responsive graphical user interface (GUI) based on PySide6, which serves as the interface between humans and the machine. Fig.\ref{overview_framework} illustrates the features of the user interface. The GUI can be installed across various platforms. In this case, the interface was installed on the Jetson AGX Orin (32GB version), an edge device and on a standard laptop with an integrated Nvidia graphics card. Subsequently, a ZED 2i stereo camera can be connected to these devices. The mobile Husky robot is utilized as the mounting platform for the components as shown in Fig.\ref{husky_robot}.


\begin{figure}[!htpb]
\centering
\includegraphics[width=2.5in]{fig1.png}
\caption{Placeholder for Husky Robot as a mounting platform for the ZED 2i camera.}
\label{husky_robot}
\end{figure}

\begin{figure}[!htpb]
\centering
\includegraphics[width=2.5in]{images/overview_eng.png}
\caption{Husky Robot as a mounting platform for the ZED 2i camera.}
\label{overview_framework}
\end{figure}

The GUI acts as a visualization and control center for the user. All data streams from the various components are bundled here. The interface displays the following visual information: 

\begin{itemize}
  \item{A live, annotated video stream of the stereo camera from the perception data pipeline.}
  \item{System performance statistics, such as frames per second (FPS) and the percentage utilization of the central processing unit (CPU), graphics processing unit (GPU), video random access memory (VRAM), and random-access memory (RAM).}
  \item{Status and system messages for active and inactive processes}
  \item{Transcribed user voice commands}
  \item{Scene descriptions and casual chat as text generated by the LLM, which are created using object data and images from the current scene}
  \item{Navigation control overview to select a recognized object to navigate to and/or to follow}
  \item{In addition, internal robot data such as motor temperatures can be queried via voice input and are displayed in the GUI}
\end{itemize}

All data and information displayed on the interface, as well as user (voice) input, can be saved locally in common file formats via the GUI, facilitating any necessary fine-tuning or training of the models (YOLO and LLM) in the future.

\subsection{Setup and Applications}
The interface provides a selection of different versions and sizes of YOLO(e) models for object detection and segmentation as well as LLMs for scene interpretation, which can be expanded as needed.  In addition, specific YOLO settings can be configured, such as showing and hiding bounding boxes, classification, and confidence. With an additional confidence slide bar, objects that are not well recognized can be filtered, which are below the set threshold. The YOLO object detection is leveraged for two functions. First, it is used for data fusion with the ZED 2i depth camera to provide object data (see Fig. \ref{data_fusion_object}), which is then fed to the LLM as a prompt. [The ZED SDK provides high-resolution stereo images and a depth map, which shows the distance of each pixel in the image. It also offers position tracking and a velocity estimation of the objects via the Python API, pyzed. This API can be used to configure camera parameters, such as resolution, FPS, depth mode, and depth measurement units. The resolution is set to 1280 x 720 pixels by default, with 60 FPS and performance mode for depth detection enabled. The unit of measurement is set to meters. The origin of the stereo camera is set to the coordinates (x=0, y=0, z=0). For compatibility reasons, the camera's 3D coordinate system has been set to match the one used in Robot Operating System 2 (ROS2), a middleware framework for robots, since the mobile robot is based on it.]
Second, it is used to navigate and follow a recognized object. To this end, the distance from each detected object to the camera and the pairwise inter-object distances within a frame are computed using the 3D Euclidean distance. The selection of the target object is initiated through its unique identification (ID), which is facilitated by either vocal input or interface selection.

In pAIrSEEption, the objects \( \mathcal{O} = \{o_1, \ldots, o_n\} \) in the scene are schematically defined as follows (for example, a recognized person standing still):

\[
o_1 = \Bigl(\text{person}, \ 
\left\{
\begin{aligned}
	& \text{ID} = 1, \\
	& \text{confidence value} = 0.95, \\
	& \text{3D position} = (x, y, z), \\
	& \text{3D speed} = (v_x = 0, v_y = 0, v_z = 0), \\
	& \text{speed classification} = \text{static}
\end{aligned}
\right\}
\Bigr)
\]

Where the object $o_1$ receives the attributes ID, confidence value, 3D positions, 3D velocity, and the classification \textit{static} if the velocity is zero. Thus, \textit{static} is an attribute derived from velocity, and the attribute set \textit{A} is expanded with this additional information. The selection of LLMs can be divided into two categories: 

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{images/framework_architecture.pdf}
	\caption{Overview of the system architecture.}
	\label{framework_architecture}
\end{figure*}


The first category is local and offline models from Ollama such as Gemma 3 or Qwen 2.5 VL, hosted on consumer PCs 
in the laboratory or on an edge device like the Jetson AGX Orin attached to the mobile Husky Robot.
This solution offers the advantage of control and data protection. In addition, quantized models can be used for weaker hardware. The second category is online, hosted models via the OpenRouter API. With a single API key, it can connect to many different models from OpenAI, Google, Meta, Anthropic, etc. Most of these are subject to a fee per query and have a request limit. In this case, factors such as server availability and data security are not within the user's purview.

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=2.5in]{images/client_server.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{client_server}
\end{figure}

If a request for a description of the scene is initiated, the Jetson or laptop transmits an image of the current scene to the Ollama server—or, depending on the selection, also to OpenRouter—with the previously merged object data as a prompt to the server for interpretation and evaluation.
The following text-prompt instructs the LLM on processing data. The goal is to enable safe navigation for a visually impaired person: 


%\begin{figure}[!htpb]
%	\centering
%	\includegraphics[width=2.5in]{images/grey_textbox.pdf}
%	\caption{Input prompt for the LLM to process with object data. Translated from German to English.}
%	\label{input_prompt}
%\end{figure}

%\begin{figure}[!htpb]
%	\centering
%	% define grey (5% black, 95% white)
%	\fcolorbox{black}{black!5}{%
%		\begin{minipage}{0.92\columnwidth} 
%			\setlength{\parskip}{0.5em} % paragraph distance
%			
%			``Describe the image briefly and precisely using the object data provided.
%			
%			\{object\_description\}
%			
%			Please describe what can be seen in the image,\\ 
%			where important objects are located,\\ 
%			and provide information about possible obstacles.''
%		\end{minipage}%
%	}
%	\caption{Input prompt for the LLM to process with object data (translated from German to English).}
%	\label{fig:llm_prompt}
%\end{figure}

%without fig. caption
%\vspace{1em} 
\noindent
\begin{center} 
	\fcolorbox{black}{black!5}{%
		\begin{minipage}{0.92\columnwidth}
			\setlength{\parskip}{0.5em}
			\ttfamily
			
			Describe the image briefly and precisely using the object data provided.
			
			\{object\_description\}
			
			Please describe what can be seen in the image,
			where important objects are located,
			and provide information about possible obstacles.
		\end{minipage}%
	}
\end{center}
%\vspace{1em} 

The object data, defined as \textit{object\_description} is provided in the prompt and has been translated into concise bullet points during the preprocessing stage. Initially, a template for the raw data was employed. Fig. \ref{} presents a schematic representation of the result of this transformation using the template. The integration of image data with readable object data through the utilization of an LLM (Language Model) facilitates the creation of a comprehensive and cohesive representation of the subject matter. 
The following description provides a detailed interpretation of a 3D scene that is accessible to humans. The result of the scene interpretation is displayed as text in the output window of the GUI and can optionally be played back via an audio file generated by activating the text-to-speech function.

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=2.5in]{fig1.png}
	\caption{Placeholder.}
	\label{transformation_template}
\end{figure}

\subsection{Navigation and motion control}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/yolo_timeline_yolo11l_tensorRT.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{yolo_timeline_tensorrt11}
\end{figure}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/timing_breakdown.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{timing_breakdown}
\end{figure}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=8.7cm]{images/yolo_performance_1_of_2.png}
	\caption{Client-server architecture between application device and Ollama server.}
	\label{yolo_performance_1_2}
\end{figure}
\section{Other Resources}
See \cite{ref1,ref2,ref3,ref4,ref5} for resources on formatting math into text and additional help in working with \LaTeX .

\section{Text}
For some of the remainer of this sample we will use dummy text to fill out paragraphs rather than use live text that may violate a copyright.

Itam, que ipiti sum dem velit la sum et dionet quatibus apitet voloritet audam, qui aliciant voloreicid quaspe volorem ut maximusandit faccum conemporerum aut ellatur, nobis arcimus.
Fugit odi ut pliquia incitium latum que cusapere perit molupta eaquaeria quod ut optatem poreiur? Quiaerr ovitior suntiant litio bearciur?

Onseque sequaes rectur autate minullore nusae nestiberum, sum voluptatio. Et ratem sequiam quaspername nos rem repudandae volum consequis nos eium aut as molupta tectum ulparumquam ut maximillesti consequas quas inctia cum volectinusa porrum unt eius cusaest exeritatur? Nias es enist fugit pa vollum reium essusam nist et pa aceaqui quo elibusdandis deligendus que nullaci lloreri bla que sa coreriam explacc atiumquos simolorpore, non prehendunt lam que occum\cite{ref6} si aut aut maximus eliaeruntia dia sequiamenime natem sendae ipidemp orehend uciisi omnienetus most verum, ommolendi omnimus, est, veni aut ipsa volendelist mo conserum volores estisciis recessi nveles ut poressitatur sitiis ex endi diti volum dolupta aut aut odi as eatquo cullabo remquis toreptum et des accus dolende pores sequas dolores tinust quas expel moditae ne sum quiatis nis endipie nihilis etum fugiae audi dia quiasit quibus.
\IEEEpubidadjcol
Ibus el et quatemo luptatque doluptaest et pe volent rem ipidusa eribus utem venimolorae dera qui acea quam etur aceruptat.
Gias anis doluptaspic tem et aliquis alique inctiuntiur?

Sedigent, si aligend elibuscid ut et ium volo tem eictore pellore ritatus ut ut ullatus in con con pere nos ab ium di tem aliqui od magnit repta volectur suntio. Nam isquiante doluptis essit, ut eos suntionsecto debitiur sum ea ipitiis adipit, oditiore, a dolorerempos aut harum ius, atquat.

Rum rem ditinti sciendunti volupiciendi sequiae nonsect oreniatur, volores sition ressimil inus solut ea volum harumqui to see\eqref{deqn_ex1a} mint aut quat eos explis ad quodi debis deliqui aspel earcius.

\begin{equation}
\label{deqn_ex1a}
x = \sum_{i=0}^{n} 2{i} Q.
\end{equation}

Alis nime volorempera perferi sitio denim repudae pre ducilit atatet volecte ssimillorae dolore, ut pel ipsa nonsequiam in re nus maiost et que dolor sunt eturita tibusanis eatent a aut et dio blaudit reptibu scipitem liquia consequodi od unto ipsae. Et enitia vel et experferum quiat harum sa net faccae dolut voloria nem. Bus ut labo. Ita eum repraer rovitia samendit aut et volupta tecupti busant omni quiae porro que nossimodic temquis anto blacita conse nis am, que ereperum eumquam quaescil imenisci quae magnimos recus ilibeaque cum etum iliate prae parumquatemo blaceaquiam quundia dit apienditem rerit re eici quaes eos sinvers pelecabo. Namendignis as exerupit aut magnim ium illabor roratecte plic tem res apiscipsam et vernat untur a deliquaest que non cus eat ea dolupiducim fugiam volum hil ius dolo eaquis sitis aut landesto quo corerest et auditaquas ditae voloribus, qui optaspis exero cusa am, ut plibus.


\section{Some Common Elements}
\subsection{Sections and Subsections}
Enumeration of section headings is desirable, but not required. When numbered, please be consistent throughout the article, that is, all headings and all levels of section headings in the article should be enumerated. Primary headings are designated with Roman numerals, secondary with capital letters, tertiary with Arabic numbers; and quaternary with lowercase letters. Reference and Acknowledgment headings are unlike all other section headings in text. They are never enumerated. They are simply primary headings without labels, regardless of whether the other headings in the article are enumerated. 

\subsection{Citations to the Bibliography}
The coding for the citations is made with the \LaTeX\ $\backslash${\tt{cite}} command. 
This will display as: see \cite{ref1}.

For multiple citations code as follows: {\tt{$\backslash$cite\{ref1,ref2,ref3\}}}
 which will produce \cite{ref1,ref2,ref3}. For reference ranges that are not consecutive code as {\tt{$\backslash$cite\{ref1,ref2,ref3,ref9\}}} which will produce  \cite{ref1,ref2,ref3,ref9}

\subsection{Lists}
In this section, we will consider three types of lists: simple unnumbered, numbered, and bulleted. There have been many options added to IEEEtran to enhance the creation of lists. If your lists are more complex than those shown below, please refer to the original ``IEEEtran\_HOWTO.pdf'' for additional options.\\

\subsubsection*{\bf A plain  unnumbered list}
\begin{list}{}{}
\item{bare\_jrnl.tex}
\item{bare\_conf.tex}
\item{bare\_jrnl\_compsoc.tex}
\item{bare\_conf\_compsoc.tex}
\item{bare\_jrnl\_comsoc.tex}
\end{list}

\subsubsection*{\bf A simple numbered list}
\begin{enumerate}
\item{bare\_jrnl.tex}
\item{bare\_conf.tex}
\item{bare\_jrnl\_compsoc.tex}
\item{bare\_conf\_compsoc.tex}
\item{bare\_jrnl\_comsoc.tex}
\end{enumerate}

\subsubsection*{\bf A simple bulleted list}
\begin{itemize}
\item{bare\_jrnl.tex}
\item{bare\_conf.tex}
\item{bare\_jrnl\_compsoc.tex}
\item{bare\_conf\_compsoc.tex}
\item{bare\_jrnl\_comsoc.tex}
\end{itemize}





\subsection{Figures}
Fig. 1 is an example of a floating figure using the graphicx package.
 Note that $\backslash${\tt{label}} must occur AFTER (or within) $\backslash${\tt{caption}}.
 For figures, $\backslash${\tt{caption}} should occur after the $\backslash${\tt{includegraphics}}.

\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{fig1}
\caption{Simulation results for the network.}
\label{fig_1}
\end{figure}

Fig. 2(a) and 2(b) is an example of a double column floating figure using two subfigures.
 (The subfig.sty package must be loaded for this to work.)
 The subfigure $\backslash${\tt{label}} commands are set within each subfloat command,
 and the $\backslash${\tt{label}} for the overall figure must come after $\backslash${\tt{caption}}.
 $\backslash${\tt{hfil}} is used as a separator to get equal spacing.
 The combined width of all the parts of the figure should do not exceed the text width or a line break will occur.
%
\begin{figure*}[!t]
\centering
\subfloat[]{\includegraphics[width=2.5in]{fig1}%
\label{fig_first_case}}
\hfil
\subfloat[]{\includegraphics[width=2.5in]{fig1}%
\label{fig_second_case}}
\caption{Dae. Ad quatur autat ut porepel itemoles dolor autem fuga. Bus quia con nessunti as remo di quatus non perum que nimus. (a) Case I. (b) Case II.}
\label{fig_sim}
\end{figure*}

Note that often IEEE papers with multi-part figures do not place the labels within the image itself (using the optional argument to $\backslash${\tt{subfloat}}[]), but instead will
 reference/describe all of them (a), (b), etc., within the main caption.
 Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
 labels, the optional argument to $\backslash${\tt{subfloat}} must be present. If a
 subcaption is not desired, leave its contents blank,
 e.g.,$\backslash${\tt{subfloat}}[].


 

\section{Tables}
Note that, for IEEE-style tables, the
 $\backslash${\tt{caption}} command should come BEFORE the table. Table captions use title case. Articles (a, an, the), coordinating conjunctions (and, but, for, or, nor), and most short prepositions are lowercase unless they are the first or last word. Table text will default to $\backslash${\tt{footnotesize}} as
 the IEEE normally uses this smaller font for tables.
 The $\backslash${\tt{label}} must come after $\backslash${\tt{caption}} as always.
 
\begin{table}[!t]
\caption{An Example of a Table\label{tab:table1}}
\centering
\begin{tabular}{|c||c|}
\hline
One & Two\\
\hline
Three & Four\\
\hline
\end{tabular}
\end{table}

\section{Algorithms}
Algorithms should be numbered and include a short title. They are set off from the text with rules above and below the title and after the last line.

\begin{algorithm}[H]
\caption{Weighted Tanimoto ELM.}\label{alg:alg1}
\begin{algorithmic}
\STATE 
\STATE {\textsc{TRAIN}}$(\mathbf{X} \mathbf{T})$
\STATE \hspace{0.5cm}$ \textbf{select randomly } W \subset \mathbf{X}  $
\STATE \hspace{0.5cm}$ N_\mathbf{t} \gets | \{ i : \mathbf{t}_i = \mathbf{t} \} | $ \textbf{ for } $ \mathbf{t}= -1,+1 $
\STATE \hspace{0.5cm}$ B_i \gets \sqrt{ \textsc{max}(N_{-1},N_{+1}) / N_{\mathbf{t}_i} } $ \textbf{ for } $ i = 1,...,N $
\STATE \hspace{0.5cm}$ \hat{\mathbf{H}} \gets  B \cdot (\mathbf{X}^T\textbf{W})/( \mathbb{1}\mathbf{X} + \mathbb{1}\textbf{W} - \mathbf{X}^T\textbf{W} ) $
\STATE \hspace{0.5cm}$ \beta \gets \left ( I/C + \hat{\mathbf{H}}^T\hat{\mathbf{H}} \right )^{-1}(\hat{\mathbf{H}}^T B\cdot \mathbf{T})  $
\STATE \hspace{0.5cm}\textbf{return}  $\textbf{W},  \beta $
\STATE 
\STATE {\textsc{PREDICT}}$(\mathbf{X} )$
\STATE \hspace{0.5cm}$ \mathbf{H} \gets  (\mathbf{X}^T\textbf{W} )/( \mathbb{1}\mathbf{X}  + \mathbb{1}\textbf{W}- \mathbf{X}^T\textbf{W}  ) $
\STATE \hspace{0.5cm}\textbf{return}  $\textsc{sign}( \mathbf{H} \beta )$
\end{algorithmic}
\label{alg1}
\end{algorithm}

Que sunt eum lam eos si dic to estist, culluptium quid qui nestrum nobis reiumquiatur minimus minctem. Ro moluptat fuga. Itatquiam ut laborpo rersped exceres vollandi repudaerem. Ulparci sunt, qui doluptaquis sumquia ndestiu sapient iorepella sunti veribus. Ro moluptat fuga. Itatquiam ut laborpo rersped exceres vollandi repudaerem. 
\section{Mathematical Typography \\ and Why It Matters}

Typographical conventions for mathematical formulas have been developed to {\bf provide uniformity and clarity of presentation across mathematical texts}. This enables the readers of those texts to both understand the author's ideas and to grasp new concepts quickly. While software such as \LaTeX \ and MathType\textsuperscript{\textregistered} can produce aesthetically pleasing math when used properly, it is also very easy to misuse the software, potentially resulting in incorrect math display.

IEEE aims to provide authors with the proper guidance on mathematical typesetting style and assist them in writing the best possible article. As such, IEEE has assembled a set of examples of good and bad mathematical typesetting \cite{ref1,ref2,ref3,ref4,ref5}. 

Further examples can be found at \url{http://journals.ieeeauthorcenter.ieee.org/wp-content/uploads/sites/7/IEEE-Math-Typesetting-Guide-for-LaTeX-Users.pdf}

\subsection{Display Equations}
The simple display equation example shown below uses the ``equation'' environment. To number the equations, use the $\backslash${\tt{label}} macro to create an identifier for the equation. LaTeX will automatically number the equation for you.
\begin{equation}
\label{deqn_ex1}
x = \sum_{i=0}^{n} 2{i} Q.
\end{equation}

\noindent is coded as follows:
\begin{verbatim}
\begin{equation}
\label{deqn_ex1}
x = \sum_{i=0}^{n} 2{i} Q.
\end{equation}
\end{verbatim}

To reference this equation in the text use the $\backslash${\tt{ref}} macro. 
Please see (\ref{deqn_ex1})\\
\noindent is coded as follows:
\begin{verbatim}
Please see (\ref{deqn_ex1})\end{verbatim}

\subsection{Equation Numbering}
{\bf{Consecutive Numbering:}} Equations within an article are numbered consecutively from the beginning of the
article to the end, i.e., (1), (2), (3), (4), (5), etc. Do not use roman numerals or section numbers for equation numbering.

\noindent {\bf{Appendix Equations:}} The continuation of consecutively numbered equations is best in the Appendix, but numbering
 as (A1), (A2), etc., is permissible.\\

\noindent {\bf{Hyphens and Periods}}: Hyphens and periods should not be used in equation numbers, i.e., use (1a) rather than
(1-a) and (2a) rather than (2.a) for subequations. This should be consistent throughout the article.

\subsection{Multi-Line Equations and Alignment}
Here we show several examples of multi-line equations and proper alignments.

\noindent {\bf{A single equation that must break over multiple lines due to length with no specific alignment.}}
\begin{multline}
\text{The first line of this example}\\
\text{The second line of this example}\\
\text{The third line of this example}
\end{multline}

\noindent is coded as:
\begin{verbatim}
\begin{multline}
\text{The first line of this example}\\
\text{The second line of this example}\\
\text{The third line of this example}
\end{multline}
\end{verbatim}

\noindent {\bf{A single equation with multiple lines aligned at the = signs}}
\begin{align}
a &= c+d \\
b &= e+f
\end{align}
\noindent is coded as:
\begin{verbatim}
\begin{align}
a &= c+d \\
b &= e+f
\end{align}
\end{verbatim}

The {\tt{align}} environment can align on multiple  points as shown in the following example:
\begin{align}
x &= y & X & =Y & a &=bc\\
x' &= y' & X' &=Y' &a' &=bz
\end{align}
\noindent is coded as:
\begin{verbatim}
\begin{align}
x &= y & X & =Y & a &=bc\\
x' &= y' & X' &=Y' &a' &=bz
\end{align}
\end{verbatim}





\subsection{Subnumbering}
The amsmath package provides a {\tt{subequations}} environment to facilitate subnumbering. An example:

\begin{subequations}\label{eq:2}
\begin{align}
f&=g \label{eq:2A}\\
f' &=g' \label{eq:2B}\\
\mathcal{L}f &= \mathcal{L}g \label{eq:2c}
\end{align}
\end{subequations}

\noindent is coded as:
\begin{verbatim}
\begin{subequations}\label{eq:2}
\begin{align}
f&=g \label{eq:2A}\\
f' &=g' \label{eq:2B}\\
\mathcal{L}f &= \mathcal{L}g \label{eq:2c}
\end{align}
\end{subequations}

\end{verbatim}

\subsection{Matrices}
There are several useful matrix environments that can save you some keystrokes. See the example coding below and the output.

\noindent {\bf{A simple matrix:}}
\begin{equation}
\begin{matrix}  0 &  1 \\ 
1 &  0 \end{matrix}
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\begin{matrix}  0 &  1 \\ 
1 &  0 \end{matrix}
\end{equation}
\end{verbatim}

\noindent {\bf{A matrix with parenthesis}}
\begin{equation}
\begin{pmatrix} 0 & -i \\
 i &  0 \end{pmatrix}
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\begin{pmatrix} 0 & -i \\
 i &  0 \end{pmatrix}
\end{equation}
\end{verbatim}

\noindent {\bf{A matrix with square brackets}}
\begin{equation}
\begin{bmatrix} 0 & -1 \\ 
1 &  0 \end{bmatrix}
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\begin{bmatrix} 0 & -1 \\ 
1 &  0 \end{bmatrix}
\end{equation}
\end{verbatim}

\noindent {\bf{A matrix with curly braces}}
\begin{equation}
\begin{Bmatrix} 1 &  0 \\ 
0 & -1 \end{Bmatrix}
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\begin{Bmatrix} 1 &  0 \\ 
0 & -1 \end{Bmatrix}
\end{equation}\end{verbatim}

\noindent {\bf{A matrix with single verticals}}
\begin{equation}
\begin{vmatrix} a &  b \\ 
c &  d \end{vmatrix}
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\begin{vmatrix} a &  b \\ 
c &  d \end{vmatrix}
\end{equation}\end{verbatim}

\noindent {\bf{A matrix with double verticals}}
\begin{equation}
\begin{Vmatrix} i &  0 \\ 
0 & -i \end{Vmatrix}
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\begin{Vmatrix} i &  0 \\ 
0 & -i \end{Vmatrix}
\end{equation}\end{verbatim}

\subsection{Arrays}
The {\tt{array}} environment allows you some options for matrix-like equations. You will have to manually key the fences, but there are other options for alignment of the columns and for setting horizontal and vertical rules. The argument to {\tt{array}} controls alignment and placement of vertical rules.

A simple array
\begin{equation}
\left(
\begin{array}{cccc}
a+b+c & uv & x-y & 27\\
a+b & u+v & z & 134
\end{array}\right)
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\left(
\begin{array}{cccc}
a+b+c & uv & x-y & 27\\
a+b & u+v & z & 134
\end{array} \right)
\end{equation}
\end{verbatim}

A slight variation on this to better align the numbers in the last column
\begin{equation}
\left(
\begin{array}{cccr}
a+b+c & uv & x-y & 27\\
a+b & u+v & z & 134
\end{array}\right)
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\left(
\begin{array}{cccr}
a+b+c & uv & x-y & 27\\
a+b & u+v & z & 134
\end{array} \right)
\end{equation}
\end{verbatim}

An array with vertical and horizontal rules
\begin{equation}
\left( \begin{array}{c|c|c|r}
a+b+c & uv & x-y & 27\\ \hline
a+b & u+v & z & 134
\end{array}\right)
\end{equation}
is coded as:
\begin{verbatim}
\begin{equation}
\left(
\begin{array}{c|c|c|r}
a+b+c & uv & x-y & 27\\
a+b & u+v & z & 134
\end{array} \right)
\end{equation}
\end{verbatim}
Note the argument now has the pipe "$\vert$" included to indicate the placement of the vertical rules.


\subsection{Cases Structures}
Many times cases can be miscoded using the wrong environment, i.e., {\tt{array}}. Using the {\tt{cases}} environment will save keystrokes (from not having to type the $\backslash${\tt{left}}$\backslash${\tt{lbrace}}) and automatically provide the correct column alignment.
\begin{equation*}
{z_m(t)} = \begin{cases}
1,&{\text{if}}\ {\beta }_m(t) \\ 
{0,}&{\text{otherwise.}} 
\end{cases}
\end{equation*}
\noindent is coded as follows:
\begin{verbatim}
\begin{equation*}
{z_m(t)} = 
\begin{cases}
1,&{\text{if}}\ {\beta }_m(t),\\ 
{0,}&{\text{otherwise.}} 
\end{cases}
\end{equation*}
\end{verbatim}
\noindent Note that the ``\&'' is used to mark the tabular alignment. This is important to get  proper column alignment. Do not use $\backslash${\tt{quad}} or other fixed spaces to try and align the columns. Also, note the use of the $\backslash${\tt{text}} macro for text elements such as ``if'' and ``otherwise.''

\subsection{Function Formatting in Equations}
Often, there is an easy way to properly format most common functions. Use of the $\backslash$ in front of the function name will in most cases, provide the correct formatting. When this does not work, the following example provides a solution using the $\backslash${\tt{text}} macro:

\begin{equation*} 
  d_{R}^{KM} = \underset {d_{l}^{KM}} {\text{arg min}} \{ d_{1}^{KM},\ldots,d_{6}^{KM}\}.
\end{equation*}

\noindent is coded as follows:
\begin{verbatim}
\begin{equation*} 
 d_{R}^{KM} = \underset {d_{l}^{KM}} 
 {\text{arg min}} \{ d_{1}^{KM},
 \ldots,d_{6}^{KM}\}.
\end{equation*}
\end{verbatim}

\subsection{ Text Acronyms Inside Equations}
This example shows where the acronym ``MSE" is coded using $\backslash${\tt{text\{\}}} to match how it appears in the text.

\begin{equation*}
 \text{MSE} = \frac {1}{n}\sum _{i=1}^{n}(Y_{i} - \hat {Y_{i}})^{2}
\end{equation*}

\begin{verbatim}
\begin{equation*}
 \text{MSE} = \frac {1}{n}\sum _{i=1}^{n}
(Y_{i} - \hat {Y_{i}})^{2}
\end{equation*}
\end{verbatim}

\section{Conclusion}
The conclusion goes here.


\section*{Acknowledgments}
This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



{\appendix[Proof of the Zonklar Equations]
Use $\backslash${\tt{appendix}} if you have a single appendix:
Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
 starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}



\section{References Section}
You can use a bibliography generated by BibTeX as a .bbl file.
 BibTeX documentation can be easily obtained at:
 http://mirror.ctan.org/biblio/bibtex/contrib/doc/
 The IEEEtran BibTeX style support page is:
 http://www.michaelshell.org/tex/ieeetran/bibtex/
 
 % argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
\section{Simple References}
You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
 (used to reserve space for the reference number labels box).

\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}

\bibitem{ref1}
{\it{Mathematics Into Type}}. American Mathematical Society. [Online]. Available: https://www.ams.org/arc/styleguide/mit-2.pdf

\bibitem{ref2}
T. W. Chaundy, P. R. Barrett and C. Batey, {\it{The Printing of Mathematics}}. London, U.K., Oxford Univ. Press, 1954.

\bibitem{ref3}
F. Mittelbach and M. Goossens, {\it{The \LaTeX Companion}}, 2nd ed. Boston, MA, USA: Pearson, 2004.

\bibitem{ref4}
G. Gr\"atzer, {\it{More Math Into LaTeX}}, New York, NY, USA: Springer, 2007.

\bibitem{ref5}M. Letourneau and J. W. Sharp, {\it{AMS-StyleGuide-online.pdf,}} American Mathematical Society, Providence, RI, USA, [Online]. Available: http://www.ams.org/arc/styleguide/index.html

\bibitem{ref6}
H. Sira-Ramirez, ``On the sliding mode control of nonlinear systems,'' \textit{Syst. Control Lett.}, vol. 19, pp. 303--312, 1992.

\bibitem{ref7}
A. Levant, ``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proc. 45th IEEE Conf. Decis.
Control}, San Diego, CA, USA, 2006, pp. 5585--5590. DOI: 10.1109/CDC.2006.377165.

\bibitem{ref8}
M. Fliess, C. Join, and H. Sira-Ramirez, ``Non-linear estimation is easy,'' \textit{Int. J. Model., Ident. Control}, vol. 4, no. 1, pp. 12--27, 2008.

\bibitem{ref9}
R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez, ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proc. Amer. Control Conf.}, Chicago, IL, USA,
2000, pp. 2245--2249.

\end{thebibliography}


\newpage

\section{Biography Section}
If you have an EPS/PDF photo (graphicx package needed), extra braces are
 needed around the contents of the optional argument to biography to prevent
 the LaTeX parser from getting confused when it sees the complicated
 $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
 your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
 simpler here.)
 
\vspace{11pt}

\bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

\vspace{11pt}

\bf{If you will not include a photo:}\vspace{-33pt}
\begin{IEEEbiographynophoto}{John Doe}
Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
\end{IEEEbiographynophoto}




\vfill

\end{document}


